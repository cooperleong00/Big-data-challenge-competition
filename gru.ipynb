{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile,datapath\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词向量特征\n",
    "#词向量训练的回调函数，用来打印loss、保存模型等等（注意，即使不训练词向量，加载时也要载入这个类）\n",
    "class train_log_word(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    def __init__(self, savename):\n",
    "        self.loss = []\n",
    "        self.time = 0.0\n",
    "        self.epoch = 0\n",
    "        self.savename = savename\n",
    "    #在每一轮开始的时候\n",
    "    def on_epoch_begin(self,model):\n",
    "        self.time = time.time()\n",
    "    #在每一轮结束的时候\n",
    "    def on_epoch_end(self,model):\n",
    "        self.epoch+=1\n",
    "        print('Epoch '+str(self.epoch)+' using time '+str(time.time()-self.time)+' seconds')\n",
    "        #这个loss似乎是总的loss叠加\n",
    "        print('loss: '+str(model.get_latest_training_loss()))\n",
    "        self.loss.append(model.get_latest_training_loss())\n",
    "        #5轮保存一次\n",
    "        if(self.epoch%5==0):\n",
    "            model.save(self.savename)\n",
    "            print(\"Model %s save done!\" % self.savename)\n",
    "        #50轮作图一次\n",
    "        if(self.epoch%50==0):\n",
    "            plt.plot(np.array(self.loss))\n",
    "            plt.title('Model loss')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            #plt.legend(['Train', 'Test'], loc='upper left')\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "#训练词向量,参数说明：存放corpus的文件，输出的文件名，轮数，词向量维数，中心词最远处理距离，线程数，过滤低于min_count的词，\n",
    "#sg=1为skip_gram训练方式、否则为cbow训练方式. alpha为初始学习率，min_alpha为最终学习率，学习率会逐渐降低至最终学习率.\n",
    "def make_w2v_model(corfile,output_model,\n",
    "                       epochs=100,vec_size=300,window=5,workers = 16,min_count=0,sg=1,alpha=0.025, min_alpha=0.0001):\n",
    "    start_time=time.time()\n",
    "    #读取语料文件为gensim需要的输入，行流式读取\n",
    "    cor=LineSentence(corfile)\n",
    "    #模型构建\n",
    "    model = gensim.models.Word2Vec(size=vec_size,window=window, workers=workers,sg=sg,min_count=min_count, \n",
    "                                   alpha = alpha,min_alpha = min_alpha)\n",
    "    #建立字典\n",
    "    model.build_vocab(cor)\n",
    "    #训练模型,注意这里使用了回调模型——train_log\n",
    "    model.train(cor,total_words=model.corpus_total_words,epochs=epochs,callbacks=[train_log_word(output_model)],compute_loss = True)\n",
    "    print(\"word2vec training complete,epochs:\"+str(epochs)+\" using time:\"+str(time.time()-start_time)+' seconds')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 将字典保存为本地文件\n",
    "def save_dict(save_dict,save_file):\n",
    "    with open(save_file,'w') as sv:\n",
    "        start = time.time()\n",
    "        sv.write(str(save_dict))\n",
    "        print('save dict using time'+str(time.time()-start))\n",
    "        \n",
    "#从本地文件加载字典\n",
    "def load_dict(dict_file):\n",
    "    with open(dict_file,'r') as df:\n",
    "        return eval(df.read())\n",
    "    \n",
    "#语料中生成字典，从1开始增序编号，如果词向量模型中没有该词则索引为0\n",
    "def make_dict(w2vmodel,cor_file,cor_length,start=0):\n",
    "    with open(cor_file) as cf:\n",
    "        all_words={}\n",
    "        cnt=1\n",
    "        for i in tqdm(range(cor_length),mininterval=1.0):\n",
    "            if i<start:\n",
    "                continue\n",
    "            for word in cf.readline().replace('\\n','').split(' '):\n",
    "                try:\n",
    "                    w2vmodel.wv[word]\n",
    "                    if word not in all_words:\n",
    "                        all_words[word]=cnt\n",
    "                        cnt+=1 \n",
    "                except:\n",
    "                    all_words[word]=0\n",
    "        return all_words\n",
    "\n",
    "# 根据字典中编号顺序保存词向量为txt文件\n",
    "def savew2v_byindex(w2vmodel,my_dict,savefile):\n",
    "    with open(savefile,'w') as sf:\n",
    "        sf.write('{0}\\n'.format(str(np.zeros(300).tolist())))\n",
    "        for key in tqdm(my_dict,mininterval=1.0):\n",
    "            sf.write('{0}\\n'.format(str(w2vmodel.wv[key].tolist())))\n",
    "            \n",
    "#从本地词向量txt读取为nparray\n",
    "def loadw2v_to_nparray(w2vfile,length):\n",
    "    with open(w2vfile) as wf:\n",
    "        w2varr = np.zeros(shape=(length,300),dtype=float)\n",
    "        line_count=0\n",
    "        for row in tqdm(range(length),mininterval=1.0):\n",
    "            #print(wf.readline())\n",
    "            #print(wf.readline())\n",
    "            w2varr[row] = np.array(list(map(float,wf.readline()[1:-2].split(','))))\n",
    "        return w2varr\n",
    "    \n",
    "\n",
    "#将train.csv中query和title分别处理输出：将词转换为索引\n",
    "def coupus_csv2nparr(trainfile,dic,end,start=0):\n",
    "    with open(trainfile,'r') as train_file:\n",
    "        t_reader = csv.reader(train_file,delimiter=',')\n",
    "        line_count=0\n",
    "        query_cor=[]\n",
    "        title_cor=[]\n",
    "        lb=[]\n",
    "        for row in tqdm(t_reader,mininterval=1.0):\n",
    "            if line_count<start:\n",
    "                line_count+=1\n",
    "                continue\n",
    "            if line_count>=end:\n",
    "                break\n",
    "            query=[]\n",
    "            for word in row[1]:\n",
    "                if word in dic:\n",
    "                    query.append(dic[word])\n",
    "                else:\n",
    "                    query.append(0)\n",
    "            query_cor.append(query)\n",
    "            title=[]\n",
    "            for word in row[3]:\n",
    "                if word in dic:\n",
    "                    title.append(dic[word])\n",
    "                else:\n",
    "                    title.append(0)\n",
    "            title_cor.append(title)\n",
    "            lb.append(int(row[4]))\n",
    "        return np.array(query_cor),np.array(title_cor),np.array(lb)\n",
    "    \n",
    "#从本地手工特征文件读取为nparray \n",
    "def exfeature_to_arr(feature_file,end,start=0):\n",
    "    with open(feature_file,'r') as ff:\n",
    "        ft_reader = csv.reader(ff,delimiter=' ')\n",
    "        ft = []\n",
    "        line_count=0\n",
    "        #for i in tqdm(range(end-start),mininterval=1.0):\n",
    "        for i in tqdm(ft_reader,mininterval=1.0):\n",
    "            if line_count<start:\n",
    "                line_count+=1\n",
    "                continue\n",
    "            if line_count>=end:\n",
    "                break\n",
    "            #ft.append(list(map(float,ff.readline().split(','))))\n",
    "            ft.append(i[0].split(','))\n",
    "            line_count+=1\n",
    "        result = np.array(ft,dtype=float)\n",
    "        print(result.shape)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_w2v = Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 42394.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36202\n",
      "save dict using time0.006981372833251953\n"
     ]
    }
   ],
   "source": [
    "my_dict = make_dict(my_w2v,'query_title.txt',20000)\n",
    "dict_len = len(my_dict)\n",
    "print(dict_len)\n",
    "save_dict(my_dict,'cor_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = load_dict('cor_dict.txt')\n",
    "dict_len = len(my_dict)\n",
    "#print(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 36202/36202 [00:10<00:00, 3480.56it/s]\n"
     ]
    }
   ],
   "source": [
    "savew2v_byindex(my_w2v,my_dict,'index_w2v.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 36203/36203 [00:07<00:00, 4933.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "w2v = loadw2v_to_nparray('index_w2v.txt',36202+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.51039243e-01 -3.58301282e-01 -1.25864401e-01  4.49612811e-02\n",
      "  9.82721567e-01 -4.56663698e-01  9.78114426e-01  8.82056475e-01\n",
      "  1.03397000e+00  2.25600198e-01 -1.39869899e-01 -4.90043014e-02\n",
      "  4.50973421e-01  6.03350811e-02  3.40391725e-01 -5.11655688e-01\n",
      " -3.38430703e-01 -5.39314687e-01 -7.88030148e-01  1.33609146e-01\n",
      "  9.64863598e-02 -1.68398581e-02  1.19910175e-02 -3.11141044e-01\n",
      " -4.60752547e-01  8.24275851e-01 -4.70046100e-04 -8.39701444e-02\n",
      "  1.62124932e-01  1.71068668e-01 -3.62478614e-01  7.34649837e-01\n",
      "  1.92381173e-01 -4.25238907e-01  5.43070793e-01  8.13569352e-02\n",
      "  3.32031190e-01  5.46799481e-01 -3.25215876e-01  1.43086031e-01\n",
      " -1.87799975e-01  6.38559222e-01 -1.93173081e-01  5.74566841e-01\n",
      "  1.20503545e+00 -3.59515101e-01 -6.80328190e-01 -5.89661717e-01\n",
      " -9.38028514e-01 -2.05821127e-01  3.64566267e-01  2.91239042e-02\n",
      "  4.34218019e-01  5.02783060e-03 -2.68455185e-02  3.79263401e-01\n",
      " -4.42996770e-01 -5.75489104e-01 -9.53654706e-01 -1.05617829e-01\n",
      "  2.27169201e-01 -2.10430339e-01 -3.52321804e-01  2.32427374e-01\n",
      " -5.58047630e-02 -3.21049094e-01  1.53189600e-01 -1.88424021e-01\n",
      "  2.22870350e-01  6.22100197e-02  2.19102189e-01  5.30951247e-02\n",
      " -3.07851672e-01 -1.64220765e-01 -2.32465714e-01 -1.33872077e-01\n",
      "  3.39071751e-01  2.64216125e-01  8.46204579e-01 -2.10148990e-01\n",
      "  9.99816656e-02  3.20312887e-01  4.02995273e-02  5.82112193e-01\n",
      "  1.61952361e-01  1.09389201e-01  4.12998170e-01  1.40657246e-01\n",
      "  6.08329058e-01 -4.95641142e-01 -2.40301713e-01 -3.03025961e-01\n",
      " -4.77422476e-01 -6.14986010e-02  5.07822558e-02  7.02588439e-01\n",
      " -1.77658036e-01  1.27803743e+00 -6.15830898e-01  1.13564998e-01\n",
      "  2.08528727e-01  4.64468658e-01  8.33074525e-02  1.92368016e-01\n",
      " -7.08014786e-01 -1.13450325e+00  6.35332286e-01  9.67629161e-03\n",
      "  1.16015583e-01 -9.31053698e-01 -1.49509996e-01 -3.61133426e-01\n",
      " -2.08229035e-01 -7.10475743e-02 -4.32130665e-01 -2.52772808e-01\n",
      "  6.43591344e-01 -1.08268902e-01  5.33966720e-01 -4.86673445e-01\n",
      "  3.16111207e-01 -1.10971302e-01  3.39580595e-01  1.00848451e-01\n",
      " -2.42266148e-01  3.14075440e-01  4.11520541e-01  5.27620137e-01\n",
      " -2.45937742e-02  2.25730434e-01  2.07383320e-01  8.15685019e-02\n",
      "  3.98431689e-01 -1.43260920e+00  4.37859446e-01 -2.21239179e-01\n",
      "  7.25446165e-01  2.55361438e-01  3.14098895e-01  2.14091033e-01\n",
      "  4.07972559e-02  2.77278423e-01  3.44134212e-01 -3.35529238e-01\n",
      "  2.86583453e-01  3.33788037e-01 -3.52598488e-01 -8.52786958e-01\n",
      "  1.10199571e-01 -2.47562960e-01 -7.22448945e-01  5.69557905e-01\n",
      " -5.43433487e-01 -1.94255665e-01 -2.56500036e-01  2.33637139e-01\n",
      " -3.68158102e-01  2.37552509e-01  2.17328593e-01  4.96186554e-01\n",
      " -6.04119718e-01  1.09621294e-01  6.03502393e-01 -7.00009108e-01\n",
      "  6.94137216e-01  1.42715827e-01  2.17396781e-01 -9.56437111e-01\n",
      "  4.15891260e-01 -3.93207431e-01 -9.58048850e-02  8.56971502e-01\n",
      " -4.46831316e-01  6.14874840e-01 -6.47716641e-01  2.06152618e-01\n",
      "  3.92435253e-01 -8.16770732e-01  1.89257767e-02  7.62475610e-01\n",
      "  6.06510878e-01  6.95808753e-02  2.46734694e-01 -3.39440495e-01\n",
      "  6.34631813e-01  6.75524235e-01  6.17476761e-01  5.22146165e-01\n",
      "  2.44197194e-02  5.97059028e-03  6.87824130e-01 -4.35190082e-01\n",
      " -3.08743924e-01  2.95344442e-01  9.63689327e-01 -6.14246190e-01\n",
      " -4.77499932e-01 -1.71995223e-01  3.99646878e-01 -2.61107702e-02\n",
      " -2.92810023e-01 -4.48321074e-01  1.20788097e+00 -4.97607142e-01\n",
      " -4.28497732e-01 -3.48948658e-01  8.36488962e-01  2.52576500e-01\n",
      "  5.20658731e-01 -1.75978839e-01  4.69290435e-01  8.75417590e-02\n",
      "  3.83262560e-02  8.96755382e-02  4.31740552e-01 -3.12101424e-01\n",
      "  2.86025945e-02 -5.29801011e-01 -4.80250537e-01  4.21768248e-01\n",
      " -3.82204115e-01  5.53194821e-01 -1.58637077e-01 -2.53994823e-01\n",
      "  1.12635410e+00 -8.30084309e-02  3.63997638e-01  3.12693566e-01\n",
      "  3.58069509e-01 -5.15454054e-01 -1.03896427e+00 -5.37055850e-01\n",
      "  7.37109184e-01 -2.97173411e-01  3.26396286e-01  7.46693537e-02\n",
      "  8.36590827e-02  5.73534310e-01 -1.80414796e-01  6.78217769e-01\n",
      " -5.53384602e-01  4.43949968e-01  2.42722780e-01  4.64141011e-01\n",
      " -3.39539051e-01  2.39023536e-01 -8.78586233e-01  9.03943121e-01\n",
      "  1.37718725e+00 -4.44420785e-01  1.50409862e-01  5.53832650e-01\n",
      "  7.03190684e-01  9.07949626e-01 -3.55145633e-01  1.27265811e-01\n",
      " -3.99248660e-01  4.52999026e-01 -9.77573171e-02 -3.95463854e-01\n",
      " -9.18977380e-01  8.68827939e-01 -1.15699694e-01  6.32100761e-01\n",
      "  6.74531385e-02 -9.51322690e-02 -4.83117253e-01 -8.08085144e-01\n",
      " -1.23729534e-01 -7.82269895e-01 -1.50148019e-01  9.97552276e-01\n",
      " -7.24027753e-01  6.71457350e-01  4.39163774e-01 -2.62811780e-01\n",
      "  4.66426402e-01  2.29645312e-01 -8.08172449e-02  3.48326504e-01\n",
      " -2.86826730e-01 -4.56285626e-01  7.83064887e-02  3.75858426e-01\n",
      " -3.02561939e-01 -1.10099387e+00 -2.26174101e-01  1.67516932e-01\n",
      " -2.96355277e-01  6.21015012e-01 -4.78583932e-01 -6.44133389e-01\n",
      " -6.80347681e-01 -2.93480784e-01 -5.02369665e-02  1.48732424e-01\n",
      "  7.23529577e-01 -4.31677341e-01 -1.89525858e-01 -7.94172823e-01]\n"
     ]
    }
   ],
   "source": [
    "print(w2v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:00, 72134.75it/s]\n"
     ]
    }
   ],
   "source": [
    "train_query,train_title,label=coupus_csv2nparr('train.csv',my_dict,20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:00, 70602.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 40)\n"
     ]
    }
   ],
   "source": [
    "exft = exfeature_to_arr('all_feature.txt',20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.80000000e+01  1.80000000e+01  2.11111111e+00  1.00000000e+00\n",
      "  1.20877854e+00  9.33196312e-01  5.60425649e-02  2.07726177e-01\n",
      "  9.94388091e+01  6.66894878e-02  1.00000000e+00  1.00000000e+00\n",
      "  1.46114556e+00  1.40855911e-01  2.66456507e-01  2.78545207e-02\n",
      "  2.38183446e-03  6.33851601e+00  1.23598145e-01  3.37712130e-01\n",
      "  3.08814360e-01  1.00000000e+00  2.51708193e-01  2.95747185e-01\n",
      "  6.37731062e-02  3.30965245e-03  3.77562289e+00  2.67254779e-01\n",
      "  3.04215459e+00  6.49082565e-02  2.33642355e-02  9.92626359e-01\n",
      "  2.64556017e+02  9.37506179e-01  1.00000000e+00  1.00000000e+00\n",
      "  9.25470452e+00  5.94897967e-01 -1.28274008e+00  9.33920912e-01]\n"
     ]
    }
   ],
   "source": [
    "print(exft[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense,Embedding,GRU,Lambda,Dropout,Concatenate,Subtract\n",
    "from keras.utils import plot_model\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_query = keras.preprocessing.sequence.pad_sequences(train_query,30)\n",
    "train_title = keras.preprocessing.sequence.pad_sequences(train_title,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class train_log(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.aucs = []\n",
    "        self.losses = []\n",
    "        self.val_losses=[]\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        #每轮训练结束后，计算auc\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        y_pred = self.model.predict([self.validation_data[0],self.validation_data[1]])\n",
    "        yp = []\n",
    "        for i in range(len(y_pred)):\n",
    "            yp.append(y_pred[i][0])\n",
    "        yt = []\n",
    "        for x in self.validation_data[2]:\n",
    "            yt.append(x[0])\n",
    "        auc = roc_auc_score(yt, yp)\n",
    "        self.aucs.append(auc)\n",
    "        print (' val-auc: ',auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "#曼哈顿距离\n",
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    return K.exp(-K.sum(K.abs(left - right), axis=1, keepdims=True))\n",
    "#欧氏距离\n",
    "def eudi_dis(left, right):\n",
    "    return K.sqrt(K.sum(K.square(left - right), axis=1, keepdims=True))\n",
    "\n",
    "####简单的双输入\n",
    "def multiinput_gru_model(vocab_length):\n",
    "    #双输入层，分别输入query与title\n",
    "    #需要分别padding所有句子至相同长度\n",
    "    input1 = Input(shape=(30,),dtype='int32',name='query_input')\n",
    "    input2 = Input(shape=(30,),dtype='int32',name='title_input')\n",
    "    #嵌入层，将输入词索引与词向量关联\n",
    "    embedding1 = Embedding(input_dim=vocab_length+1,\n",
    "                           output_dim=300,\n",
    "                           #input_length=500,\n",
    "                           weights=[w2v],\n",
    "                           mask_zero=True,\n",
    "                           trainable=False,\n",
    "                           name='query_embedding')(input1)\n",
    "    embedding2 = Embedding(input_dim=vocab_length+1,\n",
    "                           output_dim=300,\n",
    "                           #input_length=115,\n",
    "                           weights=[w2v],\n",
    "                           mask_zero=True,\n",
    "                           trainable=False,\n",
    "                           name='title_embedding')(input2)\n",
    "    #伪孪生（不共享权重）gru循环层\n",
    "    gru1= GRU(units=30,\n",
    "               #return_sequences=True,\n",
    "               #recurrent_dropout=0.1,\n",
    "              #dropout=0.5,\n",
    "               unroll=True,\n",
    "                name='query_gru')(embedding1)\n",
    "    gru2= GRU(units=30,\n",
    "               #return_sequences=True,\n",
    "               #recurrent_dropout=0.1,\n",
    "              #dropout=0.5,\n",
    "               unroll=True,\n",
    "                name='title_gru')(embedding2)\n",
    "    ########################\n",
    "    #输出句向量计算相似度层\n",
    "    mah_distance = Lambda(lambda x: exponent_neg_manhattan_distance(x[0], x[1]),\n",
    "                                output_shape=lambda x: (x[0][0], 1),name='gru_output_to_mahdis')([gru1, gru2])\n",
    "    eudi_distance = Lambda(lambda x: eudi_dis(x[0], x[1]),\n",
    "                               output_shape=lambda x: (x[0][0], 1),name='gru_output_to_eudidis')([gru1, gru2])\n",
    "    #句向量做差得关系向量，加快模型学习\n",
    "    minor_t = Subtract(name='gru_output_subtract')([gru1,gru2])\n",
    "    ###############\n",
    "    #额外特征输入\n",
    "    input3 = Input(shape=(20,),name='extra_feature_input')\n",
    "    #j将额外输入，gru输出，以及相似度，关系向量输出合并\n",
    "    concat_all = keras.layers.Concatenate(axis=-1,name='concatenate_to_dense')([gru1,gru2,mah_distance,eudi_distance,minor_t])\n",
    "    #合并层连接全连接层\n",
    "    dense1 = Dense(units=2048,\n",
    "                                activation='relu',\n",
    "                                input_shape=(902,),name='dense1')(concat_all)\n",
    "    dense1_5 = Dense(units=1024,\n",
    "                                activation='relu',name='dense2')(dense1)\n",
    "    dense1_6 = Dense(units=1024,\n",
    "                                activation='relu',name='dense3')(dense1_5)\n",
    "    dense2 = Dense(units=1024,\n",
    "                                activation='relu',name='dense4')(dense1_6)\n",
    "    dense3 = Dense(units=512,\n",
    "                                activation='relu',name='dense5')(dense2)\n",
    "    dense3_5 = Dense(units=512,\n",
    "                                activation='relu',name='dense6')(dense3)\n",
    "    dense4 = Dense(units=512,\n",
    "                                activation='relu',name='dense7')(dense3_5)\n",
    "    dense5 = Dense(units=256,\n",
    "                                activation='relu',name='dense8')(dense4)\n",
    "    dense5_5 = Dense(units=256,\n",
    "                                activation='relu',name='dense9')(dense5)\n",
    "    dense6 = Dense(units=256,\n",
    "                                activation='relu',name='dense10')(dense5)\n",
    "    dense7 = Dense(units=128,\n",
    "                                activation='relu',name='dense11')(dense6)\n",
    "    dense7_5 = Dense(units=128,\n",
    "                                activation='relu',name='dense12')(dense7)\n",
    "    dense8 = Dense(units=64,\n",
    "                                activation='relu',name='dense13')(dense7_5)\n",
    "    dense9 = Dense(units=64,\n",
    "                                activation='relu',name='dense14')(dense8)\n",
    "    dense10 = Dense(units=32,\n",
    "                                activation='relu',name='dense15')(dense9)\n",
    "    \n",
    "    #单输出，0-1\n",
    "    output = Dense(units=1,\n",
    "                                activation='sigmoid',name='output')(dense10)\n",
    "    #####\n",
    "    model = Model(inputs=[input1,input2],outputs=[output])\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0704 10:47:19.945273 13656 deprecation_wrapper.py:119] From d:\\gongjubao\\miniconda3_4.54\\envs\\huaweilab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0704 10:47:19.980178 13656 deprecation_wrapper.py:119] From d:\\gongjubao\\miniconda3_4.54\\envs\\huaweilab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0704 10:47:19.991151 13656 deprecation_wrapper.py:119] From d:\\gongjubao\\miniconda3_4.54\\envs\\huaweilab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0704 10:47:19.992149 13656 deprecation_wrapper.py:119] From d:\\gongjubao\\miniconda3_4.54\\envs\\huaweilab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0704 10:47:19.994143 13656 deprecation_wrapper.py:119] From d:\\gongjubao\\miniconda3_4.54\\envs\\huaweilab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0704 10:47:20.359204 13656 deprecation.py:323] From d:\\gongjubao\\miniconda3_4.54\\envs\\huaweilab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2888: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "query_input (InputLayer)        (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_input (InputLayer)        (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "query_embedding (Embedding)     (None, 30, 300)      10860900    query_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "title_embedding (Embedding)     (None, 30, 300)      10860900    title_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "query_gru (GRU)                 (None, 30)           29790       query_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "title_gru (GRU)                 (None, 30)           29790       title_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "gru_output_to_mahdis (Lambda)   (None, 1)            0           query_gru[0][0]                  \n",
      "                                                                 title_gru[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_output_to_eudidis (Lambda)  (None, 1)            0           query_gru[0][0]                  \n",
      "                                                                 title_gru[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_output_subtract (Subtract)  (None, 30)           0           query_gru[0][0]                  \n",
      "                                                                 title_gru[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_to_dense (Concatena (None, 92)           0           query_gru[0][0]                  \n",
      "                                                                 title_gru[0][0]                  \n",
      "                                                                 gru_output_to_mahdis[0][0]       \n",
      "                                                                 gru_output_to_eudidis[0][0]      \n",
      "                                                                 gru_output_subtract[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 2048)         190464      concatenate_to_dense[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 1024)         2098176     dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense3 (Dense)                  (None, 1024)         1049600     dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense4 (Dense)                  (None, 1024)         1049600     dense3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense5 (Dense)                  (None, 512)          524800      dense4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense6 (Dense)                  (None, 512)          262656      dense5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense7 (Dense)                  (None, 512)          262656      dense6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense8 (Dense)                  (None, 256)          131328      dense7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense10 (Dense)                 (None, 256)          65792       dense8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense11 (Dense)                 (None, 128)          32896       dense10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense12 (Dense)                 (None, 128)          16512       dense11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense13 (Dense)                 (None, 64)           8256        dense12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense14 (Dense)                 (None, 64)           4160        dense13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense15 (Dense)                 (None, 32)           2080        dense14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            33          dense15[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 27,480,389\n",
      "Trainable params: 5,758,589\n",
      "Non-trainable params: 21,721,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0704 10:47:22.272053 13656 deprecation_wrapper.py:119] From d:\\gongjubao\\miniconda3_4.54\\envs\\huaweilab\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gru_model = multiinput_gru_model(vocab_length=dict_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16200 samples, validate on 1800 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4976/16200 [========>.....................] - ETA: 1:54:12 - loss: 0.69 - ETA: 57:39 - loss: 0.6900 - ETA: 38:49 - loss: 0.67 - ETA: 29:23 - loss: 0.62 - ETA: 23:44 - loss: 0.61 - ETA: 19:57 - loss: 0.62 - ETA: 17:16 - loss: 0.61 - ETA: 15:14 - loss: 0.61 - ETA: 13:40 - loss: 0.62 - ETA: 12:24 - loss: 0.62 - ETA: 11:23 - loss: 0.62 - ETA: 10:31 - loss: 0.61 - ETA: 9:48 - loss: 0.6245 - ETA: 9:11 - loss: 0.612 - ETA: 8:38 - loss: 0.604 - ETA: 8:10 - loss: 0.608 - ETA: 7:45 - loss: 0.600 - ETA: 7:23 - loss: 0.596 - ETA: 7:03 - loss: 0.588 - ETA: 6:45 - loss: 0.596 - ETA: 6:29 - loss: 0.594 - ETA: 6:14 - loss: 0.589 - ETA: 6:00 - loss: 0.595 - ETA: 5:48 - loss: 0.597 - ETA: 5:37 - loss: 0.596 - ETA: 5:27 - loss: 0.593 - ETA: 5:17 - loss: 0.594 - ETA: 5:09 - loss: 0.596 - ETA: 5:00 - loss: 0.592 - ETA: 4:52 - loss: 0.595 - ETA: 4:45 - loss: 0.590 - ETA: 4:39 - loss: 0.594 - ETA: 4:32 - loss: 0.600 - ETA: 4:26 - loss: 0.600 - ETA: 4:21 - loss: 0.599 - ETA: 4:16 - loss: 0.599 - ETA: 4:11 - loss: 0.600 - ETA: 4:06 - loss: 0.599 - ETA: 4:02 - loss: 0.602 - ETA: 3:57 - loss: 0.605 - ETA: 3:53 - loss: 0.605 - ETA: 3:49 - loss: 0.605 - ETA: 3:46 - loss: 0.604 - ETA: 3:42 - loss: 0.605 - ETA: 3:39 - loss: 0.605 - ETA: 3:36 - loss: 0.604 - ETA: 3:33 - loss: 0.605 - ETA: 3:30 - loss: 0.607 - ETA: 3:27 - loss: 0.604 - ETA: 3:24 - loss: 0.605 - ETA: 3:22 - loss: 0.606 - ETA: 3:19 - loss: 0.606 - ETA: 3:17 - loss: 0.607 - ETA: 3:14 - loss: 0.605 - ETA: 3:12 - loss: 0.602 - ETA: 3:10 - loss: 0.600 - ETA: 3:08 - loss: 0.601 - ETA: 3:06 - loss: 0.601 - ETA: 3:04 - loss: 0.600 - ETA: 3:02 - loss: 0.600 - ETA: 3:00 - loss: 0.599 - ETA: 2:58 - loss: 0.602 - ETA: 2:56 - loss: 0.600 - ETA: 2:55 - loss: 0.600 - ETA: 2:53 - loss: 0.599 - ETA: 2:51 - loss: 0.601 - ETA: 2:50 - loss: 0.602 - ETA: 2:48 - loss: 0.603 - ETA: 2:47 - loss: 0.604 - ETA: 2:45 - loss: 0.603 - ETA: 2:44 - loss: 0.604 - ETA: 2:43 - loss: 0.604 - ETA: 2:41 - loss: 0.605 - ETA: 2:40 - loss: 0.607 - ETA: 2:39 - loss: 0.608 - ETA: 2:38 - loss: 0.608 - ETA: 2:36 - loss: 0.609 - ETA: 2:35 - loss: 0.608 - ETA: 2:34 - loss: 0.608 - ETA: 2:33 - loss: 0.608 - ETA: 2:32 - loss: 0.608 - ETA: 2:31 - loss: 0.608 - ETA: 2:30 - loss: 0.605 - ETA: 2:29 - loss: 0.603 - ETA: 2:28 - loss: 0.602 - ETA: 2:27 - loss: 0.609 - ETA: 2:26 - loss: 0.606 - ETA: 2:25 - loss: 0.607 - ETA: 2:24 - loss: 0.607 - ETA: 2:23 - loss: 0.606 - ETA: 2:22 - loss: 0.606 - ETA: 2:21 - loss: 0.606 - ETA: 2:20 - loss: 0.606 - ETA: 2:20 - loss: 0.606 - ETA: 2:19 - loss: 0.607 - ETA: 2:18 - loss: 0.607 - ETA: 2:17 - loss: 0.607 - ETA: 2:16 - loss: 0.607 - ETA: 2:16 - loss: 0.607 - ETA: 2:15 - loss: 0.607 - ETA: 2:14 - loss: 0.607 - ETA: 2:13 - loss: 0.608 - ETA: 2:13 - loss: 0.609 - ETA: 2:12 - loss: 0.609 - ETA: 2:11 - loss: 0.609 - ETA: 2:11 - loss: 0.610 - ETA: 2:10 - loss: 0.610 - ETA: 2:09 - loss: 0.610 - ETA: 2:09 - loss: 0.610 - ETA: 2:08 - loss: 0.609 - ETA: 2:07 - loss: 0.610 - ETA: 2:07 - loss: 0.612 - ETA: 2:06 - loss: 0.612 - ETA: 2:05 - loss: 0.612 - ETA: 2:05 - loss: 0.610 - ETA: 2:04 - loss: 0.609 - ETA: 2:04 - loss: 0.610 - ETA: 2:03 - loss: 0.609 - ETA: 2:03 - loss: 0.608 - ETA: 2:02 - loss: 0.608 - ETA: 2:01 - loss: 0.607 - ETA: 2:01 - loss: 0.604 - ETA: 2:00 - loss: 0.603 - ETA: 2:00 - loss: 0.605 - ETA: 1:59 - loss: 0.605 - ETA: 1:59 - loss: 0.605 - ETA: 1:58 - loss: 0.604 - ETA: 1:58 - loss: 0.603 - ETA: 1:57 - loss: 0.603 - ETA: 1:57 - loss: 0.603 - ETA: 1:56 - loss: 0.604 - ETA: 1:56 - loss: 0.603 - ETA: 1:55 - loss: 0.603 - ETA: 1:55 - loss: 0.603 - ETA: 1:54 - loss: 0.603 - ETA: 1:54 - loss: 0.603 - ETA: 1:54 - loss: 0.602 - ETA: 1:53 - loss: 0.601 - ETA: 1:53 - loss: 0.600 - ETA: 1:52 - loss: 0.599 - ETA: 1:52 - loss: 0.599 - ETA: 1:51 - loss: 0.598 - ETA: 1:51 - loss: 0.597 - ETA: 1:51 - loss: 0.595 - ETA: 1:50 - loss: 0.599 - ETA: 1:50 - loss: 0.597 - ETA: 1:49 - loss: 0.597 - ETA: 1:49 - loss: 0.598 - ETA: 1:49 - loss: 0.598 - ETA: 1:48 - loss: 0.598 - ETA: 1:48 - loss: 0.598 - ETA: 1:48 - loss: 0.598 - ETA: 1:47 - loss: 0.598 - ETA: 1:47 - loss: 0.598 - ETA: 1:46 - loss: 0.597 - ETA: 1:46 - loss: 0.597 - ETA: 1:46 - loss: 0.597 - ETA: 1:45 - loss: 0.598 - ETA: 1:45 - loss: 0.597 - ETA: 1:45 - loss: 0.598 - ETA: 1:44 - loss: 0.597 - ETA: 1:44 - loss: 0.598 - ETA: 1:44 - loss: 0.598 - ETA: 1:43 - loss: 0.598 - ETA: 1:43 - loss: 0.598 - ETA: 1:43 - loss: 0.597 - ETA: 1:42 - loss: 0.597 - ETA: 1:42 - loss: 0.597 - ETA: 1:42 - loss: 0.597 - ETA: 1:41 - loss: 0.597 - ETA: 1:41 - loss: 0.598 - ETA: 1:41 - loss: 0.597 - ETA: 1:41 - loss: 0.597 - ETA: 1:40 - loss: 0.597 - ETA: 1:40 - loss: 0.597 - ETA: 1:40 - loss: 0.597 - ETA: 1:39 - loss: 0.596 - ETA: 1:39 - loss: 0.596 - ETA: 1:39 - loss: 0.595 - ETA: 1:38 - loss: 0.595 - ETA: 1:38 - loss: 0.595 - ETA: 1:38 - loss: 0.595 - ETA: 1:38 - loss: 0.594 - ETA: 1:37 - loss: 0.593 - ETA: 1:37 - loss: 0.595 - ETA: 1:37 - loss: 0.596 - ETA: 1:37 - loss: 0.596 - ETA: 1:36 - loss: 0.596 - ETA: 1:36 - loss: 0.596 - ETA: 1:36 - loss: 0.596 - ETA: 1:35 - loss: 0.595 - ETA: 1:35 - loss: 0.595 - ETA: 1:35 - loss: 0.595 - ETA: 1:35 - loss: 0.595 - ETA: 1:34 - loss: 0.594 - ETA: 1:34 - loss: 0.594 - ETA: 1:34 - loss: 0.594 - ETA: 1:34 - loss: 0.594 - ETA: 1:34 - loss: 0.594 - ETA: 1:33 - loss: 0.594 - ETA: 1:33 - loss: 0.593 - ETA: 1:33 - loss: 0.592 - ETA: 1:33 - loss: 0.592 - ETA: 1:32 - loss: 0.591 - ETA: 1:32 - loss: 0.592 - ETA: 1:32 - loss: 0.591 - ETA: 1:32 - loss: 0.591 - ETA: 1:32 - loss: 0.590 - ETA: 1:31 - loss: 0.590 - ETA: 1:31 - loss: 0.591 - ETA: 1:31 - loss: 0.590 - ETA: 1:31 - loss: 0.590 - ETA: 1:31 - loss: 0.590 - ETA: 1:30 - loss: 0.590 - ETA: 1:30 - loss: 0.588 - ETA: 1:30 - loss: 0.588 - ETA: 1:30 - loss: 0.587 - ETA: 1:30 - loss: 0.588 - ETA: 1:29 - loss: 0.588 - ETA: 1:29 - loss: 0.588 - ETA: 1:29 - loss: 0.588 - ETA: 1:29 - loss: 0.588 - ETA: 1:29 - loss: 0.588 - ETA: 1:28 - loss: 0.587 - ETA: 1:28 - loss: 0.589 - ETA: 1:28 - loss: 0.589 - ETA: 1:28 - loss: 0.589 - ETA: 1:28 - loss: 0.589 - ETA: 1:27 - loss: 0.588 - ETA: 1:27 - loss: 0.588 - ETA: 1:27 - loss: 0.588 - ETA: 1:27 - loss: 0.587 - ETA: 1:27 - loss: 0.587 - ETA: 1:26 - loss: 0.586 - ETA: 1:26 - loss: 0.585 - ETA: 1:26 - loss: 0.584 - ETA: 1:26 - loss: 0.584 - ETA: 1:26 - loss: 0.583 - ETA: 1:25 - loss: 0.584 - ETA: 1:25 - loss: 0.583 - ETA: 1:25 - loss: 0.584 - ETA: 1:25 - loss: 0.583 - ETA: 1:25 - loss: 0.583 - ETA: 1:24 - loss: 0.582 - ETA: 1:24 - loss: 0.582 - ETA: 1:24 - loss: 0.583 - ETA: 1:24 - loss: 0.583 - ETA: 1:24 - loss: 0.584 - ETA: 1:24 - loss: 0.583 - ETA: 1:23 - loss: 0.583 - ETA: 1:23 - loss: 0.583 - ETA: 1:23 - loss: 0.583 - ETA: 1:23 - loss: 0.583 - ETA: 1:23 - loss: 0.583 - ETA: 1:22 - loss: 0.583 - ETA: 1:22 - loss: 0.583 - ETA: 1:22 - loss: 0.583 - ETA: 1:22 - loss: 0.582 - ETA: 1:22 - loss: 0.582 - ETA: 1:22 - loss: 0.583 - ETA: 1:21 - loss: 0.583 - ETA: 1:21 - loss: 0.583 - ETA: 1:21 - loss: 0.582 - ETA: 1:21 - loss: 0.582 - ETA: 1:21 - loss: 0.582 - ETA: 1:20 - loss: 0.582 - ETA: 1:20 - loss: 0.583 - ETA: 1:20 - loss: 0.582 - ETA: 1:20 - loss: 0.582 - ETA: 1:20 - loss: 0.581 - ETA: 1:20 - loss: 0.581 - ETA: 1:19 - loss: 0.581 - ETA: 1:19 - loss: 0.580 - ETA: 1:19 - loss: 0.580 - ETA: 1:19 - loss: 0.580 - ETA: 1:19 - loss: 0.581 - ETA: 1:19 - loss: 0.581 - ETA: 1:18 - loss: 0.580 - ETA: 1:18 - loss: 0.580 - ETA: 1:18 - loss: 0.580 - ETA: 1:18 - loss: 0.579 - ETA: 1:18 - loss: 0.580 - ETA: 1:18 - loss: 0.580 - ETA: 1:17 - loss: 0.579 - ETA: 1:17 - loss: 0.580 - ETA: 1:17 - loss: 0.580 - ETA: 1:17 - loss: 0.580 - ETA: 1:17 - loss: 0.580 - ETA: 1:17 - loss: 0.579 - ETA: 1:17 - loss: 0.579 - ETA: 1:16 - loss: 0.579 - ETA: 1:16 - loss: 0.580 - ETA: 1:16 - loss: 0.579 - ETA: 1:16 - loss: 0.579 - ETA: 1:16 - loss: 0.579 - ETA: 1:16 - loss: 0.579 - ETA: 1:15 - loss: 0.578 - ETA: 1:15 - loss: 0.578 - ETA: 1:15 - loss: 0.577 - ETA: 1:15 - loss: 0.578 - ETA: 1:15 - loss: 0.578 - ETA: 1:15 - loss: 0.578 - ETA: 1:14 - loss: 0.577 - ETA: 1:14 - loss: 0.577 - ETA: 1:14 - loss: 0.576 - ETA: 1:14 - loss: 0.576 - ETA: 1:14 - loss: 0.576 - ETA: 1:14 - loss: 0.576 - ETA: 1:14 - loss: 0.575 - ETA: 1:13 - loss: 0.576 - ETA: 1:13 - loss: 0.5765\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10288/16200 [==================>...........] - ETA: 1:13 - loss: 0.576 - ETA: 1:13 - loss: 0.575 - ETA: 1:13 - loss: 0.576 - ETA: 1:13 - loss: 0.576 - ETA: 1:13 - loss: 0.575 - ETA: 1:12 - loss: 0.575 - ETA: 1:12 - loss: 0.575 - ETA: 1:12 - loss: 0.575 - ETA: 1:12 - loss: 0.575 - ETA: 1:12 - loss: 0.574 - ETA: 1:12 - loss: 0.574 - ETA: 1:11 - loss: 0.574 - ETA: 1:11 - loss: 0.575 - ETA: 1:11 - loss: 0.575 - ETA: 1:11 - loss: 0.575 - ETA: 1:11 - loss: 0.575 - ETA: 1:11 - loss: 0.575 - ETA: 1:11 - loss: 0.575 - ETA: 1:10 - loss: 0.575 - ETA: 1:10 - loss: 0.575 - ETA: 1:10 - loss: 0.575 - ETA: 1:10 - loss: 0.575 - ETA: 1:10 - loss: 0.575 - ETA: 1:10 - loss: 0.575 - ETA: 1:10 - loss: 0.575 - ETA: 1:09 - loss: 0.575 - ETA: 1:09 - loss: 0.576 - ETA: 1:09 - loss: 0.577 - ETA: 1:09 - loss: 0.577 - ETA: 1:09 - loss: 0.577 - ETA: 1:09 - loss: 0.577 - ETA: 1:09 - loss: 0.577 - ETA: 1:08 - loss: 0.577 - ETA: 1:08 - loss: 0.577 - ETA: 1:08 - loss: 0.577 - ETA: 1:08 - loss: 0.577 - ETA: 1:08 - loss: 0.577 - ETA: 1:08 - loss: 0.576 - ETA: 1:08 - loss: 0.576 - ETA: 1:07 - loss: 0.577 - ETA: 1:07 - loss: 0.576 - ETA: 1:07 - loss: 0.577 - ETA: 1:07 - loss: 0.577 - ETA: 1:07 - loss: 0.576 - ETA: 1:07 - loss: 0.576 - ETA: 1:06 - loss: 0.576 - ETA: 1:06 - loss: 0.576 - ETA: 1:06 - loss: 0.576 - ETA: 1:06 - loss: 0.576 - ETA: 1:06 - loss: 0.576 - ETA: 1:06 - loss: 0.576 - ETA: 1:06 - loss: 0.576 - ETA: 1:06 - loss: 0.576 - ETA: 1:05 - loss: 0.576 - ETA: 1:05 - loss: 0.576 - ETA: 1:05 - loss: 0.575 - ETA: 1:05 - loss: 0.575 - ETA: 1:05 - loss: 0.576 - ETA: 1:05 - loss: 0.576 - ETA: 1:05 - loss: 0.576 - ETA: 1:04 - loss: 0.576 - ETA: 1:04 - loss: 0.576 - ETA: 1:04 - loss: 0.575 - ETA: 1:04 - loss: 0.576 - ETA: 1:04 - loss: 0.575 - ETA: 1:04 - loss: 0.575 - ETA: 1:04 - loss: 0.576 - ETA: 1:04 - loss: 0.576 - ETA: 1:03 - loss: 0.576 - ETA: 1:03 - loss: 0.575 - ETA: 1:03 - loss: 0.575 - ETA: 1:03 - loss: 0.575 - ETA: 1:03 - loss: 0.576 - ETA: 1:03 - loss: 0.576 - ETA: 1:03 - loss: 0.576 - ETA: 1:02 - loss: 0.576 - ETA: 1:02 - loss: 0.576 - ETA: 1:02 - loss: 0.576 - ETA: 1:02 - loss: 0.576 - ETA: 1:02 - loss: 0.576 - ETA: 1:02 - loss: 0.575 - ETA: 1:02 - loss: 0.575 - ETA: 1:02 - loss: 0.575 - ETA: 1:01 - loss: 0.575 - ETA: 1:01 - loss: 0.575 - ETA: 1:01 - loss: 0.575 - ETA: 1:01 - loss: 0.576 - ETA: 1:01 - loss: 0.575 - ETA: 1:01 - loss: 0.576 - ETA: 1:01 - loss: 0.576 - ETA: 1:01 - loss: 0.577 - ETA: 1:00 - loss: 0.576 - ETA: 1:00 - loss: 0.577 - ETA: 1:00 - loss: 0.576 - ETA: 1:00 - loss: 0.577 - ETA: 1:00 - loss: 0.577 - ETA: 1:00 - loss: 0.577 - ETA: 1:00 - loss: 0.577 - ETA: 1:00 - loss: 0.577 - ETA: 59s - loss: 0.577 - ETA: 59s - loss: 0.57 - ETA: 59s - loss: 0.57 - ETA: 59s - loss: 0.57 - ETA: 59s - loss: 0.57 - ETA: 59s - loss: 0.57 - ETA: 59s - loss: 0.57 - ETA: 59s - loss: 0.57 - ETA: 58s - loss: 0.57 - ETA: 58s - loss: 0.57 - ETA: 58s - loss: 0.57 - ETA: 58s - loss: 0.57 - ETA: 58s - loss: 0.57 - ETA: 58s - loss: 0.57 - ETA: 58s - loss: 0.57 - ETA: 58s - loss: 0.57 - ETA: 58s - loss: 0.57 - ETA: 57s - loss: 0.57 - ETA: 57s - loss: 0.57 - ETA: 57s - loss: 0.57 - ETA: 57s - loss: 0.57 - ETA: 57s - loss: 0.57 - ETA: 57s - loss: 0.57 - ETA: 57s - loss: 0.57 - ETA: 57s - loss: 0.57 - ETA: 56s - loss: 0.57 - ETA: 56s - loss: 0.57 - ETA: 56s - loss: 0.57 - ETA: 56s - loss: 0.57 - ETA: 56s - loss: 0.57 - ETA: 56s - loss: 0.57 - ETA: 56s - loss: 0.57 - ETA: 56s - loss: 0.57 - ETA: 55s - loss: 0.57 - ETA: 55s - loss: 0.57 - ETA: 55s - loss: 0.57 - ETA: 55s - loss: 0.57 - ETA: 55s - loss: 0.57 - ETA: 55s - loss: 0.57 - ETA: 55s - loss: 0.57 - ETA: 55s - loss: 0.57 - ETA: 55s - loss: 0.57 - ETA: 54s - loss: 0.57 - ETA: 54s - loss: 0.57 - ETA: 54s - loss: 0.57 - ETA: 54s - loss: 0.57 - ETA: 54s - loss: 0.57 - ETA: 54s - loss: 0.57 - ETA: 54s - loss: 0.57 - ETA: 54s - loss: 0.57 - ETA: 53s - loss: 0.57 - ETA: 53s - loss: 0.57 - ETA: 53s - loss: 0.57 - ETA: 53s - loss: 0.57 - ETA: 53s - loss: 0.57 - ETA: 53s - loss: 0.57 - ETA: 53s - loss: 0.57 - ETA: 53s - loss: 0.57 - ETA: 53s - loss: 0.57 - ETA: 52s - loss: 0.57 - ETA: 52s - loss: 0.57 - ETA: 52s - loss: 0.57 - ETA: 52s - loss: 0.57 - ETA: 52s - loss: 0.57 - ETA: 52s - loss: 0.57 - ETA: 52s - loss: 0.57 - ETA: 52s - loss: 0.57 - ETA: 52s - loss: 0.57 - ETA: 51s - loss: 0.57 - ETA: 51s - loss: 0.57 - ETA: 51s - loss: 0.57 - ETA: 51s - loss: 0.57 - ETA: 51s - loss: 0.57 - ETA: 51s - loss: 0.57 - ETA: 51s - loss: 0.57 - ETA: 51s - loss: 0.57 - ETA: 51s - loss: 0.57 - ETA: 50s - loss: 0.57 - ETA: 50s - loss: 0.57 - ETA: 50s - loss: 0.57 - ETA: 50s - loss: 0.57 - ETA: 50s - loss: 0.57 - ETA: 50s - loss: 0.57 - ETA: 50s - loss: 0.57 - ETA: 50s - loss: 0.57 - ETA: 50s - loss: 0.57 - ETA: 49s - loss: 0.57 - ETA: 49s - loss: 0.57 - ETA: 49s - loss: 0.57 - ETA: 49s - loss: 0.57 - ETA: 49s - loss: 0.57 - ETA: 49s - loss: 0.57 - ETA: 49s - loss: 0.57 - ETA: 49s - loss: 0.57 - ETA: 49s - loss: 0.57 - ETA: 48s - loss: 0.57 - ETA: 48s - loss: 0.57 - ETA: 48s - loss: 0.57 - ETA: 48s - loss: 0.57 - ETA: 48s - loss: 0.57 - ETA: 48s - loss: 0.57 - ETA: 48s - loss: 0.57 - ETA: 48s - loss: 0.57 - ETA: 48s - loss: 0.57 - ETA: 47s - loss: 0.57 - ETA: 47s - loss: 0.57 - ETA: 47s - loss: 0.57 - ETA: 47s - loss: 0.57 - ETA: 47s - loss: 0.57 - ETA: 47s - loss: 0.57 - ETA: 47s - loss: 0.57 - ETA: 47s - loss: 0.57 - ETA: 47s - loss: 0.57 - ETA: 46s - loss: 0.57 - ETA: 46s - loss: 0.57 - ETA: 46s - loss: 0.57 - ETA: 46s - loss: 0.57 - ETA: 46s - loss: 0.57 - ETA: 46s - loss: 0.57 - ETA: 46s - loss: 0.57 - ETA: 46s - loss: 0.57 - ETA: 46s - loss: 0.57 - ETA: 46s - loss: 0.57 - ETA: 45s - loss: 0.57 - ETA: 45s - loss: 0.57 - ETA: 45s - loss: 0.57 - ETA: 45s - loss: 0.57 - ETA: 45s - loss: 0.57 - ETA: 45s - loss: 0.57 - ETA: 45s - loss: 0.57 - ETA: 45s - loss: 0.57 - ETA: 45s - loss: 0.57 - ETA: 44s - loss: 0.57 - ETA: 44s - loss: 0.57 - ETA: 44s - loss: 0.57 - ETA: 44s - loss: 0.57 - ETA: 44s - loss: 0.57 - ETA: 44s - loss: 0.57 - ETA: 44s - loss: 0.57 - ETA: 44s - loss: 0.57 - ETA: 44s - loss: 0.57 - ETA: 44s - loss: 0.57 - ETA: 43s - loss: 0.57 - ETA: 43s - loss: 0.57 - ETA: 43s - loss: 0.57 - ETA: 43s - loss: 0.57 - ETA: 43s - loss: 0.57 - ETA: 43s - loss: 0.57 - ETA: 43s - loss: 0.57 - ETA: 43s - loss: 0.57 - ETA: 43s - loss: 0.57 - ETA: 42s - loss: 0.57 - ETA: 42s - loss: 0.57 - ETA: 42s - loss: 0.57 - ETA: 42s - loss: 0.57 - ETA: 42s - loss: 0.57 - ETA: 42s - loss: 0.57 - ETA: 42s - loss: 0.57 - ETA: 42s - loss: 0.57 - ETA: 42s - loss: 0.57 - ETA: 42s - loss: 0.57 - ETA: 41s - loss: 0.57 - ETA: 41s - loss: 0.57 - ETA: 41s - loss: 0.57 - ETA: 41s - loss: 0.57 - ETA: 41s - loss: 0.57 - ETA: 41s - loss: 0.57 - ETA: 41s - loss: 0.57 - ETA: 41s - loss: 0.57 - ETA: 41s - loss: 0.57 - ETA: 40s - loss: 0.57 - ETA: 40s - loss: 0.57 - ETA: 40s - loss: 0.57 - ETA: 40s - loss: 0.57 - ETA: 40s - loss: 0.57 - ETA: 40s - loss: 0.57 - ETA: 40s - loss: 0.57 - ETA: 40s - loss: 0.57 - ETA: 40s - loss: 0.57 - ETA: 40s - loss: 0.57 - ETA: 39s - loss: 0.57 - ETA: 39s - loss: 0.57 - ETA: 39s - loss: 0.57 - ETA: 39s - loss: 0.57 - ETA: 39s - loss: 0.57 - ETA: 39s - loss: 0.57 - ETA: 39s - loss: 0.57 - ETA: 39s - loss: 0.57 - ETA: 39s - loss: 0.57 - ETA: 39s - loss: 0.57 - ETA: 38s - loss: 0.57 - ETA: 38s - loss: 0.57 - ETA: 38s - loss: 0.57 - ETA: 38s - loss: 0.57 - ETA: 38s - loss: 0.57 - ETA: 38s - loss: 0.57 - ETA: 38s - loss: 0.57 - ETA: 38s - loss: 0.57 - ETA: 38s - loss: 0.57 - ETA: 37s - loss: 0.57 - ETA: 37s - loss: 0.57 - ETA: 37s - loss: 0.57 - ETA: 37s - loss: 0.57 - ETA: 37s - loss: 0.57 - ETA: 37s - loss: 0.57 - ETA: 37s - loss: 0.57 - ETA: 37s - loss: 0.57 - ETA: 37s - loss: 0.57 - ETA: 37s - loss: 0.57 - ETA: 36s - loss: 0.57 - ETA: 36s - loss: 0.57 - ETA: 36s - loss: 0.57 - ETA: 36s - loss: 0.57 - ETA: 36s - loss: 0.57 - ETA: 36s - loss: 0.57 - ETA: 36s - loss: 0.57 - ETA: 36s - loss: 0.57 - ETA: 36s - loss: 0.57 - ETA: 36s - loss: 0.57 - ETA: 35s - loss: 0.57 - ETA: 35s - loss: 0.57 - ETA: 35s - loss: 0.57 - ETA: 35s - loss: 0.57 - ETA: 35s - loss: 0.57 - ETA: 35s - loss: 0.57 - ETA: 35s - loss: 0.57 - ETA: 35s - loss: 0.57 - ETA: 35s - loss: 0.57 - ETA: 35s - loss: 0.57 - ETA: 34s - loss: 0.57 - ETA: 34s - loss: 0.57 - ETA: 34s - loss: 0.57 - ETA: 34s - loss: 0.5718"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15728/16200 [============================>.] - ETA: 34s - loss: 0.57 - ETA: 34s - loss: 0.57 - ETA: 34s - loss: 0.57 - ETA: 34s - loss: 0.57 - ETA: 34s - loss: 0.57 - ETA: 34s - loss: 0.57 - ETA: 33s - loss: 0.57 - ETA: 33s - loss: 0.57 - ETA: 33s - loss: 0.57 - ETA: 33s - loss: 0.57 - ETA: 33s - loss: 0.57 - ETA: 33s - loss: 0.57 - ETA: 33s - loss: 0.57 - ETA: 33s - loss: 0.57 - ETA: 33s - loss: 0.57 - ETA: 33s - loss: 0.57 - ETA: 32s - loss: 0.57 - ETA: 32s - loss: 0.57 - ETA: 32s - loss: 0.57 - ETA: 32s - loss: 0.57 - ETA: 32s - loss: 0.57 - ETA: 32s - loss: 0.57 - ETA: 32s - loss: 0.57 - ETA: 32s - loss: 0.57 - ETA: 32s - loss: 0.57 - ETA: 32s - loss: 0.57 - ETA: 31s - loss: 0.57 - ETA: 31s - loss: 0.57 - ETA: 31s - loss: 0.57 - ETA: 31s - loss: 0.57 - ETA: 31s - loss: 0.57 - ETA: 31s - loss: 0.57 - ETA: 31s - loss: 0.57 - ETA: 31s - loss: 0.57 - ETA: 31s - loss: 0.57 - ETA: 31s - loss: 0.57 - ETA: 30s - loss: 0.57 - ETA: 30s - loss: 0.57 - ETA: 30s - loss: 0.57 - ETA: 30s - loss: 0.57 - ETA: 30s - loss: 0.57 - ETA: 30s - loss: 0.57 - ETA: 30s - loss: 0.57 - ETA: 30s - loss: 0.57 - ETA: 30s - loss: 0.57 - ETA: 30s - loss: 0.57 - ETA: 29s - loss: 0.57 - ETA: 29s - loss: 0.57 - ETA: 29s - loss: 0.57 - ETA: 29s - loss: 0.57 - ETA: 29s - loss: 0.57 - ETA: 29s - loss: 0.57 - ETA: 29s - loss: 0.57 - ETA: 29s - loss: 0.57 - ETA: 29s - loss: 0.57 - ETA: 29s - loss: 0.57 - ETA: 28s - loss: 0.57 - ETA: 28s - loss: 0.57 - ETA: 28s - loss: 0.57 - ETA: 28s - loss: 0.57 - ETA: 28s - loss: 0.57 - ETA: 28s - loss: 0.57 - ETA: 28s - loss: 0.57 - ETA: 28s - loss: 0.57 - ETA: 28s - loss: 0.57 - ETA: 28s - loss: 0.57 - ETA: 27s - loss: 0.57 - ETA: 27s - loss: 0.57 - ETA: 27s - loss: 0.57 - ETA: 27s - loss: 0.57 - ETA: 27s - loss: 0.57 - ETA: 27s - loss: 0.57 - ETA: 27s - loss: 0.57 - ETA: 27s - loss: 0.57 - ETA: 27s - loss: 0.57 - ETA: 27s - loss: 0.57 - ETA: 26s - loss: 0.57 - ETA: 26s - loss: 0.57 - ETA: 26s - loss: 0.57 - ETA: 26s - loss: 0.57 - ETA: 26s - loss: 0.57 - ETA: 26s - loss: 0.57 - ETA: 26s - loss: 0.57 - ETA: 26s - loss: 0.57 - ETA: 26s - loss: 0.57 - ETA: 26s - loss: 0.57 - ETA: 26s - loss: 0.57 - ETA: 25s - loss: 0.57 - ETA: 25s - loss: 0.57 - ETA: 25s - loss: 0.57 - ETA: 25s - loss: 0.57 - ETA: 25s - loss: 0.57 - ETA: 25s - loss: 0.57 - ETA: 25s - loss: 0.57 - ETA: 25s - loss: 0.57 - ETA: 25s - loss: 0.57 - ETA: 25s - loss: 0.57 - ETA: 24s - loss: 0.57 - ETA: 24s - loss: 0.57 - ETA: 24s - loss: 0.57 - ETA: 24s - loss: 0.57 - ETA: 24s - loss: 0.57 - ETA: 24s - loss: 0.57 - ETA: 24s - loss: 0.57 - ETA: 24s - loss: 0.57 - ETA: 24s - loss: 0.57 - ETA: 24s - loss: 0.57 - ETA: 23s - loss: 0.57 - ETA: 23s - loss: 0.57 - ETA: 23s - loss: 0.57 - ETA: 23s - loss: 0.57 - ETA: 23s - loss: 0.57 - ETA: 23s - loss: 0.57 - ETA: 23s - loss: 0.57 - ETA: 23s - loss: 0.57 - ETA: 23s - loss: 0.57 - ETA: 23s - loss: 0.57 - ETA: 23s - loss: 0.57 - ETA: 22s - loss: 0.57 - ETA: 22s - loss: 0.57 - ETA: 22s - loss: 0.57 - ETA: 22s - loss: 0.57 - ETA: 22s - loss: 0.57 - ETA: 22s - loss: 0.57 - ETA: 22s - loss: 0.57 - ETA: 22s - loss: 0.57 - ETA: 22s - loss: 0.57 - ETA: 22s - loss: 0.57 - ETA: 21s - loss: 0.57 - ETA: 21s - loss: 0.57 - ETA: 21s - loss: 0.57 - ETA: 21s - loss: 0.57 - ETA: 21s - loss: 0.57 - ETA: 21s - loss: 0.57 - ETA: 21s - loss: 0.57 - ETA: 21s - loss: 0.57 - ETA: 21s - loss: 0.57 - ETA: 21s - loss: 0.57 - ETA: 21s - loss: 0.57 - ETA: 20s - loss: 0.57 - ETA: 20s - loss: 0.57 - ETA: 20s - loss: 0.57 - ETA: 20s - loss: 0.57 - ETA: 20s - loss: 0.57 - ETA: 20s - loss: 0.57 - ETA: 20s - loss: 0.57 - ETA: 20s - loss: 0.57 - ETA: 20s - loss: 0.57 - ETA: 20s - loss: 0.57 - ETA: 19s - loss: 0.57 - ETA: 19s - loss: 0.57 - ETA: 19s - loss: 0.57 - ETA: 19s - loss: 0.57 - ETA: 19s - loss: 0.57 - ETA: 19s - loss: 0.57 - ETA: 19s - loss: 0.57 - ETA: 19s - loss: 0.57 - ETA: 19s - loss: 0.57 - ETA: 19s - loss: 0.57 - ETA: 19s - loss: 0.57 - ETA: 18s - loss: 0.57 - ETA: 18s - loss: 0.57 - ETA: 18s - loss: 0.57 - ETA: 18s - loss: 0.57 - ETA: 18s - loss: 0.57 - ETA: 18s - loss: 0.57 - ETA: 18s - loss: 0.57 - ETA: 18s - loss: 0.57 - ETA: 18s - loss: 0.57 - ETA: 18s - loss: 0.57 - ETA: 18s - loss: 0.57 - ETA: 17s - loss: 0.57 - ETA: 17s - loss: 0.57 - ETA: 17s - loss: 0.57 - ETA: 17s - loss: 0.57 - ETA: 17s - loss: 0.57 - ETA: 17s - loss: 0.57 - ETA: 17s - loss: 0.57 - ETA: 17s - loss: 0.57 - ETA: 17s - loss: 0.57 - ETA: 17s - loss: 0.57 - ETA: 16s - loss: 0.57 - ETA: 16s - loss: 0.57 - ETA: 16s - loss: 0.57 - ETA: 16s - loss: 0.57 - ETA: 16s - loss: 0.57 - ETA: 16s - loss: 0.57 - ETA: 16s - loss: 0.57 - ETA: 16s - loss: 0.57 - ETA: 16s - loss: 0.57 - ETA: 16s - loss: 0.57 - ETA: 16s - loss: 0.57 - ETA: 15s - loss: 0.57 - ETA: 15s - loss: 0.57 - ETA: 15s - loss: 0.57 - ETA: 15s - loss: 0.57 - ETA: 15s - loss: 0.57 - ETA: 15s - loss: 0.57 - ETA: 15s - loss: 0.57 - ETA: 15s - loss: 0.57 - ETA: 15s - loss: 0.57 - ETA: 15s - loss: 0.57 - ETA: 15s - loss: 0.57 - ETA: 14s - loss: 0.57 - ETA: 14s - loss: 0.57 - ETA: 14s - loss: 0.57 - ETA: 14s - loss: 0.57 - ETA: 14s - loss: 0.57 - ETA: 14s - loss: 0.57 - ETA: 14s - loss: 0.57 - ETA: 14s - loss: 0.57 - ETA: 14s - loss: 0.57 - ETA: 14s - loss: 0.57 - ETA: 14s - loss: 0.57 - ETA: 13s - loss: 0.57 - ETA: 13s - loss: 0.57 - ETA: 13s - loss: 0.57 - ETA: 13s - loss: 0.57 - ETA: 13s - loss: 0.57 - ETA: 13s - loss: 0.57 - ETA: 13s - loss: 0.57 - ETA: 13s - loss: 0.57 - ETA: 13s - loss: 0.57 - ETA: 13s - loss: 0.57 - ETA: 13s - loss: 0.57 - ETA: 12s - loss: 0.57 - ETA: 12s - loss: 0.57 - ETA: 12s - loss: 0.57 - ETA: 12s - loss: 0.57 - ETA: 12s - loss: 0.57 - ETA: 12s - loss: 0.57 - ETA: 12s - loss: 0.57 - ETA: 12s - loss: 0.57 - ETA: 12s - loss: 0.57 - ETA: 12s - loss: 0.57 - ETA: 12s - loss: 0.57 - ETA: 11s - loss: 0.57 - ETA: 11s - loss: 0.57 - ETA: 11s - loss: 0.57 - ETA: 11s - loss: 0.57 - ETA: 11s - loss: 0.57 - ETA: 11s - loss: 0.57 - ETA: 11s - loss: 0.57 - ETA: 11s - loss: 0.57 - ETA: 11s - loss: 0.57 - ETA: 11s - loss: 0.57 - ETA: 11s - loss: 0.57 - ETA: 10s - loss: 0.57 - ETA: 10s - loss: 0.57 - ETA: 10s - loss: 0.57 - ETA: 10s - loss: 0.57 - ETA: 10s - loss: 0.57 - ETA: 10s - loss: 0.57 - ETA: 10s - loss: 0.57 - ETA: 10s - loss: 0.57 - ETA: 10s - loss: 0.57 - ETA: 10s - loss: 0.57 - ETA: 10s - loss: 0.57 - ETA: 9s - loss: 0.5704 - ETA: 9s - loss: 0.570 - ETA: 9s - loss: 0.570 - ETA: 9s - loss: 0.570 - ETA: 9s - loss: 0.570 - ETA: 9s - loss: 0.570 - ETA: 9s - loss: 0.570 - ETA: 9s - loss: 0.570 - ETA: 9s - loss: 0.570 - ETA: 9s - loss: 0.570 - ETA: 9s - loss: 0.570 - ETA: 8s - loss: 0.570 - ETA: 8s - loss: 0.570 - ETA: 8s - loss: 0.570 - ETA: 8s - loss: 0.570 - ETA: 8s - loss: 0.570 - ETA: 8s - loss: 0.570 - ETA: 8s - loss: 0.570 - ETA: 8s - loss: 0.569 - ETA: 8s - loss: 0.569 - ETA: 8s - loss: 0.569 - ETA: 8s - loss: 0.569 - ETA: 7s - loss: 0.569 - ETA: 7s - loss: 0.569 - ETA: 7s - loss: 0.569 - ETA: 7s - loss: 0.569 - ETA: 7s - loss: 0.569 - ETA: 7s - loss: 0.569 - ETA: 7s - loss: 0.569 - ETA: 7s - loss: 0.569 - ETA: 7s - loss: 0.569 - ETA: 7s - loss: 0.569 - ETA: 7s - loss: 0.569 - ETA: 6s - loss: 0.569 - ETA: 6s - loss: 0.569 - ETA: 6s - loss: 0.570 - ETA: 6s - loss: 0.569 - ETA: 6s - loss: 0.569 - ETA: 6s - loss: 0.569 - ETA: 6s - loss: 0.570 - ETA: 6s - loss: 0.570 - ETA: 6s - loss: 0.570 - ETA: 6s - loss: 0.570 - ETA: 6s - loss: 0.570 - ETA: 5s - loss: 0.570 - ETA: 5s - loss: 0.570 - ETA: 5s - loss: 0.570 - ETA: 5s - loss: 0.570 - ETA: 5s - loss: 0.570 - ETA: 5s - loss: 0.570 - ETA: 5s - loss: 0.570 - ETA: 5s - loss: 0.570 - ETA: 5s - loss: 0.570 - ETA: 5s - loss: 0.570 - ETA: 5s - loss: 0.570 - ETA: 4s - loss: 0.570 - ETA: 4s - loss: 0.570 - ETA: 4s - loss: 0.571 - ETA: 4s - loss: 0.570 - ETA: 4s - loss: 0.571 - ETA: 4s - loss: 0.571 - ETA: 4s - loss: 0.571 - ETA: 4s - loss: 0.571 - ETA: 4s - loss: 0.570 - ETA: 4s - loss: 0.571 - ETA: 4s - loss: 0.571 - ETA: 3s - loss: 0.571 - ETA: 3s - loss: 0.571 - ETA: 3s - loss: 0.571 - ETA: 3s - loss: 0.571 - ETA: 3s - loss: 0.571 - ETA: 3s - loss: 0.571 - ETA: 3s - loss: 0.571 - ETA: 3s - loss: 0.571 - ETA: 3s - loss: 0.571 - ETA: 3s - loss: 0.571 - ETA: 3s - loss: 0.571 - ETA: 2s - loss: 0.571 - ETA: 2s - loss: 0.571 - ETA: 2s - loss: 0.571 - ETA: 2s - loss: 0.571 - ETA: 2s - loss: 0.5709"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16200/16200 [==============================] - ETA: 2s - loss: 0.570 - ETA: 2s - loss: 0.571 - ETA: 2s - loss: 0.571 - ETA: 2s - loss: 0.571 - ETA: 2s - loss: 0.571 - ETA: 2s - loss: 0.571 - ETA: 2s - loss: 0.571 - ETA: 1s - loss: 0.571 - ETA: 1s - loss: 0.571 - ETA: 1s - loss: 0.571 - ETA: 1s - loss: 0.571 - ETA: 1s - loss: 0.570 - ETA: 1s - loss: 0.570 - ETA: 1s - loss: 0.570 - ETA: 1s - loss: 0.570 - ETA: 1s - loss: 0.571 - ETA: 1s - loss: 0.570 - ETA: 1s - loss: 0.570 - ETA: 0s - loss: 0.570 - ETA: 0s - loss: 0.570 - ETA: 0s - loss: 0.570 - ETA: 0s - loss: 0.570 - ETA: 0s - loss: 0.570 - ETA: 0s - loss: 0.570 - ETA: 0s - loss: 0.570 - ETA: 0s - loss: 0.570 - ETA: 0s - loss: 0.570 - ETA: 0s - loss: 0.570 - ETA: 0s - loss: 0.570 - 92s 6ms/step - loss: 0.5708 - val_loss: 0.5492\n",
      " val-auc:  0.5236883223754121\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5024/16200 [========>.....................] - ETA: 1:23 - loss: 0.632 - ETA: 1:22 - loss: 0.597 - ETA: 1:23 - loss: 0.515 - ETA: 1:22 - loss: 0.562 - ETA: 1:23 - loss: 0.520 - ETA: 1:23 - loss: 0.503 - ETA: 1:24 - loss: 0.522 - ETA: 1:25 - loss: 0.536 - ETA: 1:25 - loss: 0.539 - ETA: 1:26 - loss: 0.548 - ETA: 1:25 - loss: 0.543 - ETA: 1:25 - loss: 0.545 - ETA: 1:25 - loss: 0.546 - ETA: 1:24 - loss: 0.542 - ETA: 1:24 - loss: 0.543 - ETA: 1:24 - loss: 0.540 - ETA: 1:24 - loss: 0.546 - ETA: 1:24 - loss: 0.538 - ETA: 1:25 - loss: 0.540 - ETA: 1:25 - loss: 0.545 - ETA: 1:25 - loss: 0.542 - ETA: 1:24 - loss: 0.543 - ETA: 1:24 - loss: 0.535 - ETA: 1:24 - loss: 0.542 - ETA: 1:24 - loss: 0.546 - ETA: 1:24 - loss: 0.546 - ETA: 1:24 - loss: 0.542 - ETA: 1:24 - loss: 0.540 - ETA: 1:24 - loss: 0.548 - ETA: 1:24 - loss: 0.551 - ETA: 1:24 - loss: 0.554 - ETA: 1:24 - loss: 0.554 - ETA: 1:24 - loss: 0.556 - ETA: 1:24 - loss: 0.558 - ETA: 1:24 - loss: 0.560 - ETA: 1:23 - loss: 0.561 - ETA: 1:23 - loss: 0.562 - ETA: 1:23 - loss: 0.561 - ETA: 1:23 - loss: 0.557 - ETA: 1:23 - loss: 0.557 - ETA: 1:23 - loss: 0.556 - ETA: 1:23 - loss: 0.558 - ETA: 1:23 - loss: 0.562 - ETA: 1:23 - loss: 0.562 - ETA: 1:23 - loss: 0.561 - ETA: 1:22 - loss: 0.558 - ETA: 1:23 - loss: 0.557 - ETA: 1:23 - loss: 0.560 - ETA: 1:23 - loss: 0.557 - ETA: 1:23 - loss: 0.555 - ETA: 1:23 - loss: 0.555 - ETA: 1:23 - loss: 0.555 - ETA: 1:23 - loss: 0.555 - ETA: 1:23 - loss: 0.556 - ETA: 1:23 - loss: 0.555 - ETA: 1:22 - loss: 0.553 - ETA: 1:22 - loss: 0.556 - ETA: 1:22 - loss: 0.556 - ETA: 1:22 - loss: 0.556 - ETA: 1:22 - loss: 0.557 - ETA: 1:22 - loss: 0.557 - ETA: 1:22 - loss: 0.557 - ETA: 1:22 - loss: 0.555 - ETA: 1:21 - loss: 0.555 - ETA: 1:21 - loss: 0.555 - ETA: 1:21 - loss: 0.556 - ETA: 1:21 - loss: 0.554 - ETA: 1:21 - loss: 0.555 - ETA: 1:21 - loss: 0.557 - ETA: 1:21 - loss: 0.557 - ETA: 1:21 - loss: 0.557 - ETA: 1:21 - loss: 0.559 - ETA: 1:20 - loss: 0.558 - ETA: 1:20 - loss: 0.560 - ETA: 1:20 - loss: 0.560 - ETA: 1:20 - loss: 0.559 - ETA: 1:20 - loss: 0.560 - ETA: 1:20 - loss: 0.557 - ETA: 1:20 - loss: 0.558 - ETA: 1:20 - loss: 0.558 - ETA: 1:20 - loss: 0.558 - ETA: 1:20 - loss: 0.557 - ETA: 1:20 - loss: 0.556 - ETA: 1:19 - loss: 0.557 - ETA: 1:19 - loss: 0.557 - ETA: 1:19 - loss: 0.555 - ETA: 1:19 - loss: 0.557 - ETA: 1:19 - loss: 0.557 - ETA: 1:19 - loss: 0.560 - ETA: 1:19 - loss: 0.560 - ETA: 1:19 - loss: 0.558 - ETA: 1:19 - loss: 0.557 - ETA: 1:19 - loss: 0.557 - ETA: 1:18 - loss: 0.555 - ETA: 1:18 - loss: 0.558 - ETA: 1:18 - loss: 0.557 - ETA: 1:18 - loss: 0.557 - ETA: 1:18 - loss: 0.557 - ETA: 1:18 - loss: 0.558 - ETA: 1:18 - loss: 0.558 - ETA: 1:18 - loss: 0.558 - ETA: 1:18 - loss: 0.558 - ETA: 1:18 - loss: 0.558 - ETA: 1:18 - loss: 0.559 - ETA: 1:18 - loss: 0.558 - ETA: 1:18 - loss: 0.559 - ETA: 1:17 - loss: 0.557 - ETA: 1:17 - loss: 0.558 - ETA: 1:17 - loss: 0.559 - ETA: 1:17 - loss: 0.559 - ETA: 1:17 - loss: 0.559 - ETA: 1:17 - loss: 0.561 - ETA: 1:17 - loss: 0.562 - ETA: 1:17 - loss: 0.563 - ETA: 1:17 - loss: 0.562 - ETA: 1:17 - loss: 0.564 - ETA: 1:17 - loss: 0.564 - ETA: 1:16 - loss: 0.565 - ETA: 1:16 - loss: 0.565 - ETA: 1:16 - loss: 0.565 - ETA: 1:16 - loss: 0.565 - ETA: 1:16 - loss: 0.564 - ETA: 1:16 - loss: 0.566 - ETA: 1:16 - loss: 0.565 - ETA: 1:16 - loss: 0.564 - ETA: 1:16 - loss: 0.563 - ETA: 1:16 - loss: 0.563 - ETA: 1:16 - loss: 0.565 - ETA: 1:15 - loss: 0.563 - ETA: 1:15 - loss: 0.563 - ETA: 1:15 - loss: 0.564 - ETA: 1:15 - loss: 0.566 - ETA: 1:15 - loss: 0.566 - ETA: 1:15 - loss: 0.566 - ETA: 1:15 - loss: 0.566 - ETA: 1:15 - loss: 0.566 - ETA: 1:15 - loss: 0.566 - ETA: 1:15 - loss: 0.566 - ETA: 1:15 - loss: 0.566 - ETA: 1:15 - loss: 0.566 - ETA: 1:15 - loss: 0.567 - ETA: 1:14 - loss: 0.568 - ETA: 1:14 - loss: 0.567 - ETA: 1:14 - loss: 0.568 - ETA: 1:14 - loss: 0.567 - ETA: 1:14 - loss: 0.567 - ETA: 1:14 - loss: 0.568 - ETA: 1:14 - loss: 0.567 - ETA: 1:14 - loss: 0.567 - ETA: 1:14 - loss: 0.566 - ETA: 1:14 - loss: 0.567 - ETA: 1:14 - loss: 0.567 - ETA: 1:14 - loss: 0.567 - ETA: 1:14 - loss: 0.567 - ETA: 1:13 - loss: 0.568 - ETA: 1:13 - loss: 0.567 - ETA: 1:13 - loss: 0.567 - ETA: 1:13 - loss: 0.567 - ETA: 1:13 - loss: 0.567 - ETA: 1:13 - loss: 0.567 - ETA: 1:13 - loss: 0.566 - ETA: 1:13 - loss: 0.566 - ETA: 1:13 - loss: 0.566 - ETA: 1:13 - loss: 0.565 - ETA: 1:13 - loss: 0.565 - ETA: 1:12 - loss: 0.566 - ETA: 1:12 - loss: 0.567 - ETA: 1:12 - loss: 0.567 - ETA: 1:12 - loss: 0.567 - ETA: 1:12 - loss: 0.569 - ETA: 1:12 - loss: 0.568 - ETA: 1:12 - loss: 0.568 - ETA: 1:12 - loss: 0.567 - ETA: 1:12 - loss: 0.567 - ETA: 1:12 - loss: 0.567 - ETA: 1:12 - loss: 0.566 - ETA: 1:11 - loss: 0.565 - ETA: 1:11 - loss: 0.564 - ETA: 1:11 - loss: 0.565 - ETA: 1:11 - loss: 0.564 - ETA: 1:11 - loss: 0.563 - ETA: 1:11 - loss: 0.562 - ETA: 1:11 - loss: 0.563 - ETA: 1:11 - loss: 0.563 - ETA: 1:11 - loss: 0.563 - ETA: 1:11 - loss: 0.564 - ETA: 1:11 - loss: 0.565 - ETA: 1:11 - loss: 0.566 - ETA: 1:10 - loss: 0.566 - ETA: 1:10 - loss: 0.566 - ETA: 1:10 - loss: 0.566 - ETA: 1:10 - loss: 0.565 - ETA: 1:10 - loss: 0.565 - ETA: 1:10 - loss: 0.564 - ETA: 1:10 - loss: 0.566 - ETA: 1:10 - loss: 0.566 - ETA: 1:10 - loss: 0.565 - ETA: 1:10 - loss: 0.565 - ETA: 1:10 - loss: 0.564 - ETA: 1:10 - loss: 0.565 - ETA: 1:09 - loss: 0.564 - ETA: 1:09 - loss: 0.564 - ETA: 1:09 - loss: 0.566 - ETA: 1:09 - loss: 0.565 - ETA: 1:09 - loss: 0.566 - ETA: 1:09 - loss: 0.565 - ETA: 1:09 - loss: 0.566 - ETA: 1:09 - loss: 0.566 - ETA: 1:09 - loss: 0.565 - ETA: 1:09 - loss: 0.565 - ETA: 1:09 - loss: 0.565 - ETA: 1:09 - loss: 0.565 - ETA: 1:09 - loss: 0.564 - ETA: 1:08 - loss: 0.564 - ETA: 1:08 - loss: 0.563 - ETA: 1:08 - loss: 0.562 - ETA: 1:08 - loss: 0.562 - ETA: 1:08 - loss: 0.561 - ETA: 1:08 - loss: 0.560 - ETA: 1:08 - loss: 0.560 - ETA: 1:08 - loss: 0.560 - ETA: 1:08 - loss: 0.560 - ETA: 1:08 - loss: 0.560 - ETA: 1:08 - loss: 0.560 - ETA: 1:08 - loss: 0.559 - ETA: 1:08 - loss: 0.558 - ETA: 1:07 - loss: 0.559 - ETA: 1:07 - loss: 0.558 - ETA: 1:07 - loss: 0.559 - ETA: 1:07 - loss: 0.559 - ETA: 1:07 - loss: 0.560 - ETA: 1:07 - loss: 0.560 - ETA: 1:07 - loss: 0.559 - ETA: 1:07 - loss: 0.559 - ETA: 1:07 - loss: 0.559 - ETA: 1:07 - loss: 0.558 - ETA: 1:07 - loss: 0.559 - ETA: 1:07 - loss: 0.559 - ETA: 1:07 - loss: 0.558 - ETA: 1:06 - loss: 0.557 - ETA: 1:06 - loss: 0.558 - ETA: 1:06 - loss: 0.557 - ETA: 1:06 - loss: 0.558 - ETA: 1:06 - loss: 0.557 - ETA: 1:06 - loss: 0.557 - ETA: 1:06 - loss: 0.557 - ETA: 1:06 - loss: 0.558 - ETA: 1:06 - loss: 0.557 - ETA: 1:06 - loss: 0.557 - ETA: 1:06 - loss: 0.558 - ETA: 1:05 - loss: 0.558 - ETA: 1:05 - loss: 0.558 - ETA: 1:05 - loss: 0.558 - ETA: 1:05 - loss: 0.558 - ETA: 1:05 - loss: 0.557 - ETA: 1:05 - loss: 0.558 - ETA: 1:05 - loss: 0.558 - ETA: 1:05 - loss: 0.559 - ETA: 1:05 - loss: 0.558 - ETA: 1:05 - loss: 0.558 - ETA: 1:05 - loss: 0.558 - ETA: 1:04 - loss: 0.558 - ETA: 1:04 - loss: 0.557 - ETA: 1:04 - loss: 0.558 - ETA: 1:04 - loss: 0.558 - ETA: 1:04 - loss: 0.558 - ETA: 1:04 - loss: 0.558 - ETA: 1:04 - loss: 0.558 - ETA: 1:04 - loss: 0.558 - ETA: 1:04 - loss: 0.558 - ETA: 1:04 - loss: 0.558 - ETA: 1:04 - loss: 0.558 - ETA: 1:04 - loss: 0.557 - ETA: 1:03 - loss: 0.557 - ETA: 1:03 - loss: 0.558 - ETA: 1:03 - loss: 0.559 - ETA: 1:03 - loss: 0.559 - ETA: 1:03 - loss: 0.559 - ETA: 1:03 - loss: 0.558 - ETA: 1:03 - loss: 0.559 - ETA: 1:03 - loss: 0.558 - ETA: 1:03 - loss: 0.558 - ETA: 1:03 - loss: 0.558 - ETA: 1:03 - loss: 0.558 - ETA: 1:02 - loss: 0.558 - ETA: 1:02 - loss: 0.558 - ETA: 1:02 - loss: 0.557 - ETA: 1:02 - loss: 0.557 - ETA: 1:02 - loss: 0.557 - ETA: 1:02 - loss: 0.557 - ETA: 1:02 - loss: 0.558 - ETA: 1:02 - loss: 0.558 - ETA: 1:02 - loss: 0.558 - ETA: 1:02 - loss: 0.559 - ETA: 1:02 - loss: 0.559 - ETA: 1:02 - loss: 0.559 - ETA: 1:01 - loss: 0.559 - ETA: 1:01 - loss: 0.558 - ETA: 1:01 - loss: 0.559 - ETA: 1:01 - loss: 0.559 - ETA: 1:01 - loss: 0.559 - ETA: 1:01 - loss: 0.558 - ETA: 1:01 - loss: 0.559 - ETA: 1:01 - loss: 0.558 - ETA: 1:01 - loss: 0.559 - ETA: 1:01 - loss: 0.559 - ETA: 1:01 - loss: 0.559 - ETA: 1:01 - loss: 0.558 - ETA: 1:00 - loss: 0.558 - ETA: 1:00 - loss: 0.558 - ETA: 1:00 - loss: 0.558 - ETA: 1:00 - loss: 0.558 - ETA: 1:00 - loss: 0.557 - ETA: 1:00 - loss: 0.5574"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10448/16200 [==================>...........] - ETA: 1:00 - loss: 0.557 - ETA: 1:00 - loss: 0.558 - ETA: 1:00 - loss: 0.558 - ETA: 1:00 - loss: 0.557 - ETA: 1:00 - loss: 0.558 - ETA: 59s - loss: 0.557 - ETA: 59s - loss: 0.55 - ETA: 59s - loss: 0.55 - ETA: 59s - loss: 0.55 - ETA: 59s - loss: 0.55 - ETA: 59s - loss: 0.55 - ETA: 59s - loss: 0.55 - ETA: 59s - loss: 0.55 - ETA: 59s - loss: 0.55 - ETA: 59s - loss: 0.55 - ETA: 58s - loss: 0.55 - ETA: 58s - loss: 0.55 - ETA: 58s - loss: 0.55 - ETA: 58s - loss: 0.55 - ETA: 58s - loss: 0.55 - ETA: 58s - loss: 0.55 - ETA: 58s - loss: 0.55 - ETA: 58s - loss: 0.55 - ETA: 58s - loss: 0.55 - ETA: 58s - loss: 0.55 - ETA: 58s - loss: 0.55 - ETA: 57s - loss: 0.55 - ETA: 57s - loss: 0.55 - ETA: 57s - loss: 0.55 - ETA: 57s - loss: 0.55 - ETA: 57s - loss: 0.55 - ETA: 57s - loss: 0.55 - ETA: 57s - loss: 0.55 - ETA: 57s - loss: 0.55 - ETA: 57s - loss: 0.55 - ETA: 57s - loss: 0.55 - ETA: 57s - loss: 0.55 - ETA: 57s - loss: 0.56 - ETA: 56s - loss: 0.55 - ETA: 56s - loss: 0.56 - ETA: 56s - loss: 0.56 - ETA: 56s - loss: 0.56 - ETA: 56s - loss: 0.56 - ETA: 56s - loss: 0.56 - ETA: 56s - loss: 0.56 - ETA: 56s - loss: 0.56 - ETA: 56s - loss: 0.56 - ETA: 56s - loss: 0.55 - ETA: 56s - loss: 0.55 - ETA: 55s - loss: 0.55 - ETA: 55s - loss: 0.55 - ETA: 55s - loss: 0.55 - ETA: 55s - loss: 0.55 - ETA: 55s - loss: 0.55 - ETA: 55s - loss: 0.55 - ETA: 55s - loss: 0.55 - ETA: 55s - loss: 0.55 - ETA: 55s - loss: 0.55 - ETA: 55s - loss: 0.55 - ETA: 55s - loss: 0.55 - ETA: 55s - loss: 0.55 - ETA: 54s - loss: 0.55 - ETA: 54s - loss: 0.55 - ETA: 54s - loss: 0.55 - ETA: 54s - loss: 0.55 - ETA: 54s - loss: 0.55 - ETA: 54s - loss: 0.55 - ETA: 54s - loss: 0.55 - ETA: 54s - loss: 0.55 - ETA: 54s - loss: 0.55 - ETA: 54s - loss: 0.55 - ETA: 54s - loss: 0.55 - ETA: 54s - loss: 0.55 - ETA: 53s - loss: 0.56 - ETA: 53s - loss: 0.56 - ETA: 53s - loss: 0.56 - ETA: 53s - loss: 0.56 - ETA: 53s - loss: 0.56 - ETA: 53s - loss: 0.56 - ETA: 53s - loss: 0.56 - ETA: 53s - loss: 0.56 - ETA: 53s - loss: 0.56 - ETA: 53s - loss: 0.56 - ETA: 53s - loss: 0.56 - ETA: 53s - loss: 0.56 - ETA: 52s - loss: 0.56 - ETA: 52s - loss: 0.56 - ETA: 52s - loss: 0.56 - ETA: 52s - loss: 0.56 - ETA: 52s - loss: 0.56 - ETA: 52s - loss: 0.56 - ETA: 52s - loss: 0.56 - ETA: 52s - loss: 0.56 - ETA: 52s - loss: 0.56 - ETA: 52s - loss: 0.56 - ETA: 52s - loss: 0.56 - ETA: 52s - loss: 0.56 - ETA: 51s - loss: 0.56 - ETA: 51s - loss: 0.56 - ETA: 51s - loss: 0.56 - ETA: 51s - loss: 0.56 - ETA: 51s - loss: 0.56 - ETA: 51s - loss: 0.56 - ETA: 51s - loss: 0.56 - ETA: 51s - loss: 0.56 - ETA: 51s - loss: 0.56 - ETA: 51s - loss: 0.56 - ETA: 51s - loss: 0.56 - ETA: 51s - loss: 0.56 - ETA: 50s - loss: 0.56 - ETA: 50s - loss: 0.56 - ETA: 50s - loss: 0.56 - ETA: 50s - loss: 0.56 - ETA: 50s - loss: 0.56 - ETA: 50s - loss: 0.56 - ETA: 50s - loss: 0.56 - ETA: 50s - loss: 0.56 - ETA: 50s - loss: 0.56 - ETA: 50s - loss: 0.56 - ETA: 50s - loss: 0.56 - ETA: 50s - loss: 0.56 - ETA: 49s - loss: 0.56 - ETA: 49s - loss: 0.56 - ETA: 49s - loss: 0.56 - ETA: 49s - loss: 0.56 - ETA: 49s - loss: 0.56 - ETA: 49s - loss: 0.56 - ETA: 49s - loss: 0.56 - ETA: 49s - loss: 0.56 - ETA: 49s - loss: 0.56 - ETA: 49s - loss: 0.56 - ETA: 49s - loss: 0.56 - ETA: 48s - loss: 0.56 - ETA: 48s - loss: 0.56 - ETA: 48s - loss: 0.56 - ETA: 48s - loss: 0.56 - ETA: 48s - loss: 0.56 - ETA: 48s - loss: 0.56 - ETA: 48s - loss: 0.56 - ETA: 48s - loss: 0.56 - ETA: 48s - loss: 0.56 - ETA: 48s - loss: 0.56 - ETA: 48s - loss: 0.56 - ETA: 48s - loss: 0.56 - ETA: 47s - loss: 0.56 - ETA: 47s - loss: 0.56 - ETA: 47s - loss: 0.56 - ETA: 47s - loss: 0.56 - ETA: 47s - loss: 0.56 - ETA: 47s - loss: 0.56 - ETA: 47s - loss: 0.56 - ETA: 47s - loss: 0.56 - ETA: 47s - loss: 0.56 - ETA: 47s - loss: 0.56 - ETA: 47s - loss: 0.56 - ETA: 46s - loss: 0.56 - ETA: 46s - loss: 0.56 - ETA: 46s - loss: 0.56 - ETA: 46s - loss: 0.56 - ETA: 46s - loss: 0.56 - ETA: 46s - loss: 0.56 - ETA: 46s - loss: 0.56 - ETA: 46s - loss: 0.56 - ETA: 46s - loss: 0.56 - ETA: 46s - loss: 0.56 - ETA: 46s - loss: 0.56 - ETA: 46s - loss: 0.56 - ETA: 45s - loss: 0.56 - ETA: 45s - loss: 0.56 - ETA: 45s - loss: 0.56 - ETA: 45s - loss: 0.56 - ETA: 45s - loss: 0.56 - ETA: 45s - loss: 0.56 - ETA: 45s - loss: 0.56 - ETA: 45s - loss: 0.56 - ETA: 45s - loss: 0.56 - ETA: 45s - loss: 0.56 - ETA: 45s - loss: 0.56 - ETA: 45s - loss: 0.56 - ETA: 44s - loss: 0.56 - ETA: 44s - loss: 0.56 - ETA: 44s - loss: 0.56 - ETA: 44s - loss: 0.56 - ETA: 44s - loss: 0.56 - ETA: 44s - loss: 0.56 - ETA: 44s - loss: 0.56 - ETA: 44s - loss: 0.56 - ETA: 44s - loss: 0.56 - ETA: 44s - loss: 0.56 - ETA: 44s - loss: 0.56 - ETA: 43s - loss: 0.56 - ETA: 43s - loss: 0.56 - ETA: 43s - loss: 0.56 - ETA: 43s - loss: 0.56 - ETA: 43s - loss: 0.56 - ETA: 43s - loss: 0.56 - ETA: 43s - loss: 0.56 - ETA: 43s - loss: 0.56 - ETA: 43s - loss: 0.56 - ETA: 43s - loss: 0.56 - ETA: 43s - loss: 0.56 - ETA: 43s - loss: 0.56 - ETA: 42s - loss: 0.56 - ETA: 42s - loss: 0.56 - ETA: 42s - loss: 0.56 - ETA: 42s - loss: 0.56 - ETA: 42s - loss: 0.56 - ETA: 42s - loss: 0.56 - ETA: 42s - loss: 0.56 - ETA: 42s - loss: 0.56 - ETA: 42s - loss: 0.56 - ETA: 42s - loss: 0.56 - ETA: 42s - loss: 0.56 - ETA: 41s - loss: 0.56 - ETA: 41s - loss: 0.56 - ETA: 41s - loss: 0.56 - ETA: 41s - loss: 0.56 - ETA: 41s - loss: 0.56 - ETA: 41s - loss: 0.56 - ETA: 41s - loss: 0.56 - ETA: 41s - loss: 0.56 - ETA: 41s - loss: 0.56 - ETA: 41s - loss: 0.56 - ETA: 41s - loss: 0.56 - ETA: 41s - loss: 0.56 - ETA: 40s - loss: 0.56 - ETA: 40s - loss: 0.56 - ETA: 40s - loss: 0.56 - ETA: 40s - loss: 0.56 - ETA: 40s - loss: 0.56 - ETA: 40s - loss: 0.56 - ETA: 40s - loss: 0.56 - ETA: 40s - loss: 0.56 - ETA: 40s - loss: 0.56 - ETA: 40s - loss: 0.56 - ETA: 40s - loss: 0.56 - ETA: 40s - loss: 0.56 - ETA: 39s - loss: 0.56 - ETA: 39s - loss: 0.56 - ETA: 39s - loss: 0.56 - ETA: 39s - loss: 0.56 - ETA: 39s - loss: 0.56 - ETA: 39s - loss: 0.57 - ETA: 39s - loss: 0.57 - ETA: 39s - loss: 0.57 - ETA: 39s - loss: 0.57 - ETA: 39s - loss: 0.57 - ETA: 39s - loss: 0.57 - ETA: 38s - loss: 0.56 - ETA: 38s - loss: 0.57 - ETA: 38s - loss: 0.57 - ETA: 38s - loss: 0.56 - ETA: 38s - loss: 0.57 - ETA: 38s - loss: 0.56 - ETA: 38s - loss: 0.57 - ETA: 38s - loss: 0.57 - ETA: 38s - loss: 0.57 - ETA: 38s - loss: 0.57 - ETA: 38s - loss: 0.57 - ETA: 38s - loss: 0.57 - ETA: 37s - loss: 0.57 - ETA: 37s - loss: 0.57 - ETA: 37s - loss: 0.57 - ETA: 37s - loss: 0.57 - ETA: 37s - loss: 0.57 - ETA: 37s - loss: 0.57 - ETA: 37s - loss: 0.56 - ETA: 37s - loss: 0.56 - ETA: 37s - loss: 0.56 - ETA: 37s - loss: 0.56 - ETA: 37s - loss: 0.56 - ETA: 36s - loss: 0.56 - ETA: 36s - loss: 0.56 - ETA: 36s - loss: 0.56 - ETA: 36s - loss: 0.56 - ETA: 36s - loss: 0.56 - ETA: 36s - loss: 0.56 - ETA: 36s - loss: 0.56 - ETA: 36s - loss: 0.56 - ETA: 36s - loss: 0.56 - ETA: 36s - loss: 0.56 - ETA: 36s - loss: 0.56 - ETA: 35s - loss: 0.56 - ETA: 35s - loss: 0.56 - ETA: 35s - loss: 0.56 - ETA: 35s - loss: 0.56 - ETA: 35s - loss: 0.56 - ETA: 35s - loss: 0.56 - ETA: 35s - loss: 0.56 - ETA: 35s - loss: 0.56 - ETA: 35s - loss: 0.56 - ETA: 35s - loss: 0.56 - ETA: 35s - loss: 0.56 - ETA: 34s - loss: 0.56 - ETA: 34s - loss: 0.56 - ETA: 34s - loss: 0.56 - ETA: 34s - loss: 0.56 - ETA: 34s - loss: 0.56 - ETA: 34s - loss: 0.56 - ETA: 34s - loss: 0.56 - ETA: 34s - loss: 0.56 - ETA: 34s - loss: 0.56 - ETA: 34s - loss: 0.56 - ETA: 34s - loss: 0.56 - ETA: 34s - loss: 0.56 - ETA: 33s - loss: 0.56 - ETA: 33s - loss: 0.56 - ETA: 33s - loss: 0.56 - ETA: 33s - loss: 0.56 - ETA: 33s - loss: 0.56 - ETA: 33s - loss: 0.56 - ETA: 33s - loss: 0.57 - ETA: 33s - loss: 0.57 - ETA: 33s - loss: 0.57 - ETA: 33s - loss: 0.56 - ETA: 33s - loss: 0.56 - ETA: 32s - loss: 0.56 - ETA: 32s - loss: 0.56 - ETA: 32s - loss: 0.56 - ETA: 32s - loss: 0.56 - ETA: 32s - loss: 0.56 - ETA: 32s - loss: 0.56 - ETA: 32s - loss: 0.56 - ETA: 32s - loss: 0.56 - ETA: 32s - loss: 0.56 - ETA: 32s - loss: 0.56 - ETA: 32s - loss: 0.56 - ETA: 31s - loss: 0.56 - ETA: 31s - loss: 0.56 - ETA: 31s - loss: 0.56 - ETA: 31s - loss: 0.56 - ETA: 31s - loss: 0.56 - ETA: 31s - loss: 0.56 - ETA: 31s - loss: 0.56 - ETA: 31s - loss: 0.56 - ETA: 31s - loss: 0.56 - ETA: 31s - loss: 0.56 - ETA: 31s - loss: 0.56 - ETA: 31s - loss: 0.5696"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15888/16200 [============================>.] - ETA: 30s - loss: 0.56 - ETA: 30s - loss: 0.56 - ETA: 30s - loss: 0.56 - ETA: 30s - loss: 0.56 - ETA: 30s - loss: 0.56 - ETA: 30s - loss: 0.56 - ETA: 30s - loss: 0.56 - ETA: 30s - loss: 0.56 - ETA: 30s - loss: 0.56 - ETA: 30s - loss: 0.56 - ETA: 30s - loss: 0.56 - ETA: 30s - loss: 0.56 - ETA: 29s - loss: 0.56 - ETA: 29s - loss: 0.56 - ETA: 29s - loss: 0.56 - ETA: 29s - loss: 0.56 - ETA: 29s - loss: 0.56 - ETA: 29s - loss: 0.56 - ETA: 29s - loss: 0.56 - ETA: 29s - loss: 0.56 - ETA: 29s - loss: 0.56 - ETA: 29s - loss: 0.56 - ETA: 29s - loss: 0.56 - ETA: 29s - loss: 0.56 - ETA: 28s - loss: 0.56 - ETA: 28s - loss: 0.56 - ETA: 28s - loss: 0.56 - ETA: 28s - loss: 0.56 - ETA: 28s - loss: 0.56 - ETA: 28s - loss: 0.56 - ETA: 28s - loss: 0.56 - ETA: 28s - loss: 0.56 - ETA: 28s - loss: 0.56 - ETA: 28s - loss: 0.56 - ETA: 28s - loss: 0.56 - ETA: 28s - loss: 0.56 - ETA: 27s - loss: 0.56 - ETA: 27s - loss: 0.56 - ETA: 27s - loss: 0.56 - ETA: 27s - loss: 0.56 - ETA: 27s - loss: 0.56 - ETA: 27s - loss: 0.56 - ETA: 27s - loss: 0.56 - ETA: 27s - loss: 0.56 - ETA: 27s - loss: 0.56 - ETA: 27s - loss: 0.56 - ETA: 27s - loss: 0.56 - ETA: 27s - loss: 0.56 - ETA: 26s - loss: 0.56 - ETA: 26s - loss: 0.56 - ETA: 26s - loss: 0.56 - ETA: 26s - loss: 0.56 - ETA: 26s - loss: 0.56 - ETA: 26s - loss: 0.56 - ETA: 26s - loss: 0.56 - ETA: 26s - loss: 0.56 - ETA: 26s - loss: 0.56 - ETA: 26s - loss: 0.56 - ETA: 26s - loss: 0.56 - ETA: 26s - loss: 0.56 - ETA: 25s - loss: 0.56 - ETA: 25s - loss: 0.56 - ETA: 25s - loss: 0.56 - ETA: 25s - loss: 0.56 - ETA: 25s - loss: 0.56 - ETA: 25s - loss: 0.56 - ETA: 25s - loss: 0.56 - ETA: 25s - loss: 0.56 - ETA: 25s - loss: 0.56 - ETA: 25s - loss: 0.56 - ETA: 25s - loss: 0.56 - ETA: 25s - loss: 0.56 - ETA: 24s - loss: 0.56 - ETA: 24s - loss: 0.56 - ETA: 24s - loss: 0.56 - ETA: 24s - loss: 0.56 - ETA: 24s - loss: 0.56 - ETA: 24s - loss: 0.56 - ETA: 24s - loss: 0.56 - ETA: 24s - loss: 0.56 - ETA: 24s - loss: 0.56 - ETA: 24s - loss: 0.56 - ETA: 24s - loss: 0.56 - ETA: 24s - loss: 0.56 - ETA: 23s - loss: 0.56 - ETA: 23s - loss: 0.56 - ETA: 23s - loss: 0.56 - ETA: 23s - loss: 0.56 - ETA: 23s - loss: 0.56 - ETA: 23s - loss: 0.56 - ETA: 23s - loss: 0.56 - ETA: 23s - loss: 0.56 - ETA: 23s - loss: 0.56 - ETA: 23s - loss: 0.56 - ETA: 23s - loss: 0.56 - ETA: 22s - loss: 0.56 - ETA: 22s - loss: 0.56 - ETA: 22s - loss: 0.56 - ETA: 22s - loss: 0.56 - ETA: 22s - loss: 0.56 - ETA: 22s - loss: 0.56 - ETA: 22s - loss: 0.56 - ETA: 22s - loss: 0.56 - ETA: 22s - loss: 0.56 - ETA: 22s - loss: 0.56 - ETA: 22s - loss: 0.56 - ETA: 22s - loss: 0.56 - ETA: 21s - loss: 0.56 - ETA: 21s - loss: 0.56 - ETA: 21s - loss: 0.56 - ETA: 21s - loss: 0.56 - ETA: 21s - loss: 0.56 - ETA: 21s - loss: 0.56 - ETA: 21s - loss: 0.56 - ETA: 21s - loss: 0.56 - ETA: 21s - loss: 0.56 - ETA: 21s - loss: 0.56 - ETA: 21s - loss: 0.56 - ETA: 21s - loss: 0.56 - ETA: 20s - loss: 0.56 - ETA: 20s - loss: 0.56 - ETA: 20s - loss: 0.56 - ETA: 20s - loss: 0.56 - ETA: 20s - loss: 0.56 - ETA: 20s - loss: 0.56 - ETA: 20s - loss: 0.56 - ETA: 20s - loss: 0.56 - ETA: 20s - loss: 0.56 - ETA: 20s - loss: 0.56 - ETA: 20s - loss: 0.56 - ETA: 19s - loss: 0.56 - ETA: 19s - loss: 0.56 - ETA: 19s - loss: 0.56 - ETA: 19s - loss: 0.56 - ETA: 19s - loss: 0.56 - ETA: 19s - loss: 0.56 - ETA: 19s - loss: 0.56 - ETA: 19s - loss: 0.56 - ETA: 19s - loss: 0.56 - ETA: 19s - loss: 0.56 - ETA: 19s - loss: 0.56 - ETA: 19s - loss: 0.56 - ETA: 18s - loss: 0.56 - ETA: 18s - loss: 0.56 - ETA: 18s - loss: 0.56 - ETA: 18s - loss: 0.56 - ETA: 18s - loss: 0.56 - ETA: 18s - loss: 0.56 - ETA: 18s - loss: 0.56 - ETA: 18s - loss: 0.56 - ETA: 18s - loss: 0.56 - ETA: 18s - loss: 0.56 - ETA: 18s - loss: 0.56 - ETA: 18s - loss: 0.56 - ETA: 17s - loss: 0.56 - ETA: 17s - loss: 0.56 - ETA: 17s - loss: 0.56 - ETA: 17s - loss: 0.56 - ETA: 17s - loss: 0.56 - ETA: 17s - loss: 0.56 - ETA: 17s - loss: 0.56 - ETA: 17s - loss: 0.56 - ETA: 17s - loss: 0.56 - ETA: 17s - loss: 0.56 - ETA: 17s - loss: 0.56 - ETA: 16s - loss: 0.56 - ETA: 16s - loss: 0.56 - ETA: 16s - loss: 0.56 - ETA: 16s - loss: 0.56 - ETA: 16s - loss: 0.56 - ETA: 16s - loss: 0.56 - ETA: 16s - loss: 0.56 - ETA: 16s - loss: 0.56 - ETA: 16s - loss: 0.56 - ETA: 16s - loss: 0.56 - ETA: 16s - loss: 0.56 - ETA: 15s - loss: 0.56 - ETA: 15s - loss: 0.56 - ETA: 15s - loss: 0.56 - ETA: 15s - loss: 0.56 - ETA: 15s - loss: 0.56 - ETA: 15s - loss: 0.56 - ETA: 15s - loss: 0.56 - ETA: 15s - loss: 0.56 - ETA: 15s - loss: 0.56 - ETA: 15s - loss: 0.56 - ETA: 15s - loss: 0.56 - ETA: 14s - loss: 0.56 - ETA: 14s - loss: 0.56 - ETA: 14s - loss: 0.56 - ETA: 14s - loss: 0.56 - ETA: 14s - loss: 0.56 - ETA: 14s - loss: 0.56 - ETA: 14s - loss: 0.56 - ETA: 14s - loss: 0.56 - ETA: 14s - loss: 0.56 - ETA: 14s - loss: 0.56 - ETA: 14s - loss: 0.56 - ETA: 14s - loss: 0.56 - ETA: 13s - loss: 0.56 - ETA: 13s - loss: 0.56 - ETA: 13s - loss: 0.56 - ETA: 13s - loss: 0.56 - ETA: 13s - loss: 0.56 - ETA: 13s - loss: 0.56 - ETA: 13s - loss: 0.56 - ETA: 13s - loss: 0.56 - ETA: 13s - loss: 0.56 - ETA: 13s - loss: 0.56 - ETA: 13s - loss: 0.56 - ETA: 12s - loss: 0.56 - ETA: 12s - loss: 0.56 - ETA: 12s - loss: 0.56 - ETA: 12s - loss: 0.56 - ETA: 12s - loss: 0.56 - ETA: 12s - loss: 0.56 - ETA: 12s - loss: 0.56 - ETA: 12s - loss: 0.56 - ETA: 12s - loss: 0.56 - ETA: 12s - loss: 0.56 - ETA: 12s - loss: 0.56 - ETA: 12s - loss: 0.56 - ETA: 11s - loss: 0.56 - ETA: 11s - loss: 0.56 - ETA: 11s - loss: 0.56 - ETA: 11s - loss: 0.56 - ETA: 11s - loss: 0.56 - ETA: 11s - loss: 0.56 - ETA: 11s - loss: 0.56 - ETA: 11s - loss: 0.56 - ETA: 11s - loss: 0.56 - ETA: 11s - loss: 0.56 - ETA: 11s - loss: 0.56 - ETA: 10s - loss: 0.56 - ETA: 10s - loss: 0.56 - ETA: 10s - loss: 0.56 - ETA: 10s - loss: 0.56 - ETA: 10s - loss: 0.56 - ETA: 10s - loss: 0.56 - ETA: 10s - loss: 0.56 - ETA: 10s - loss: 0.56 - ETA: 10s - loss: 0.56 - ETA: 10s - loss: 0.56 - ETA: 10s - loss: 0.56 - ETA: 9s - loss: 0.5667 - ETA: 9s - loss: 0.566 - ETA: 9s - loss: 0.566 - ETA: 9s - loss: 0.566 - ETA: 9s - loss: 0.566 - ETA: 9s - loss: 0.566 - ETA: 9s - loss: 0.566 - ETA: 9s - loss: 0.566 - ETA: 9s - loss: 0.566 - ETA: 9s - loss: 0.566 - ETA: 9s - loss: 0.566 - ETA: 9s - loss: 0.566 - ETA: 8s - loss: 0.566 - ETA: 8s - loss: 0.566 - ETA: 8s - loss: 0.566 - ETA: 8s - loss: 0.566 - ETA: 8s - loss: 0.566 - ETA: 8s - loss: 0.566 - ETA: 8s - loss: 0.566 - ETA: 8s - loss: 0.566 - ETA: 8s - loss: 0.567 - ETA: 8s - loss: 0.567 - ETA: 8s - loss: 0.567 - ETA: 7s - loss: 0.567 - ETA: 7s - loss: 0.567 - ETA: 7s - loss: 0.567 - ETA: 7s - loss: 0.567 - ETA: 7s - loss: 0.567 - ETA: 7s - loss: 0.566 - ETA: 7s - loss: 0.567 - ETA: 7s - loss: 0.567 - ETA: 7s - loss: 0.567 - ETA: 7s - loss: 0.567 - ETA: 7s - loss: 0.567 - ETA: 7s - loss: 0.567 - ETA: 6s - loss: 0.566 - ETA: 6s - loss: 0.566 - ETA: 6s - loss: 0.566 - ETA: 6s - loss: 0.566 - ETA: 6s - loss: 0.566 - ETA: 6s - loss: 0.566 - ETA: 6s - loss: 0.566 - ETA: 6s - loss: 0.566 - ETA: 6s - loss: 0.566 - ETA: 6s - loss: 0.566 - ETA: 6s - loss: 0.566 - ETA: 5s - loss: 0.566 - ETA: 5s - loss: 0.566 - ETA: 5s - loss: 0.566 - ETA: 5s - loss: 0.566 - ETA: 5s - loss: 0.566 - ETA: 5s - loss: 0.566 - ETA: 5s - loss: 0.566 - ETA: 5s - loss: 0.566 - ETA: 5s - loss: 0.566 - ETA: 5s - loss: 0.566 - ETA: 5s - loss: 0.566 - ETA: 5s - loss: 0.566 - ETA: 4s - loss: 0.567 - ETA: 4s - loss: 0.566 - ETA: 4s - loss: 0.566 - ETA: 4s - loss: 0.566 - ETA: 4s - loss: 0.566 - ETA: 4s - loss: 0.566 - ETA: 4s - loss: 0.566 - ETA: 4s - loss: 0.566 - ETA: 4s - loss: 0.567 - ETA: 4s - loss: 0.567 - ETA: 4s - loss: 0.567 - ETA: 3s - loss: 0.566 - ETA: 3s - loss: 0.566 - ETA: 3s - loss: 0.566 - ETA: 3s - loss: 0.566 - ETA: 3s - loss: 0.566 - ETA: 3s - loss: 0.566 - ETA: 3s - loss: 0.566 - ETA: 3s - loss: 0.566 - ETA: 3s - loss: 0.566 - ETA: 3s - loss: 0.566 - ETA: 3s - loss: 0.566 - ETA: 3s - loss: 0.566 - ETA: 2s - loss: 0.566 - ETA: 2s - loss: 0.566 - ETA: 2s - loss: 0.566 - ETA: 2s - loss: 0.566 - ETA: 2s - loss: 0.566 - ETA: 2s - loss: 0.566 - ETA: 2s - loss: 0.566 - ETA: 2s - loss: 0.566 - ETA: 2s - loss: 0.566 - ETA: 2s - loss: 0.566 - ETA: 2s - loss: 0.566 - ETA: 1s - loss: 0.566 - ETA: 1s - loss: 0.566 - ETA: 1s - loss: 0.566 - ETA: 1s - loss: 0.5660"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16200/16200 [==============================] - ETA: 1s - loss: 0.566 - ETA: 1s - loss: 0.566 - ETA: 1s - loss: 0.566 - ETA: 1s - loss: 0.566 - ETA: 1s - loss: 0.566 - ETA: 1s - loss: 0.566 - ETA: 1s - loss: 0.566 - ETA: 1s - loss: 0.566 - ETA: 0s - loss: 0.566 - ETA: 0s - loss: 0.566 - ETA: 0s - loss: 0.566 - ETA: 0s - loss: 0.566 - ETA: 0s - loss: 0.566 - ETA: 0s - loss: 0.566 - ETA: 0s - loss: 0.566 - ETA: 0s - loss: 0.566 - ETA: 0s - loss: 0.566 - ETA: 0s - loss: 0.566 - ETA: 0s - loss: 0.566 - 89s 6ms/step - loss: 0.5662 - val_loss: 0.5494\n",
      " val-auc:  0.5117493738937939\n",
      "using time190.8397831916809\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start=time.time()\n",
    "my_log = train_log()\n",
    "gru_his = gru_model.fit(x=[train_query[:18000],train_title[:18000]],\n",
    "                   y=[label[:18000]],\n",
    "                   epochs=2,\n",
    "                   validation_split=0.1,\n",
    "                        batch_size=16,\n",
    "                     callbacks=[#keras.callbacks.EarlyStopping(monitor='val_loss',patience=3, verbose=2, mode='min'),\n",
    "                               keras.callbacks.TerminateOnNaN(),\n",
    "                                 my_log\n",
    "                                #keras.callbacks.ModelCheckpoint('model/sia-gru.model', monitor='val_loss', verbose=1, save_best_only=True,mode='auto')\n",
    "                             ])\n",
    "#callbacks=[Histories(),keras.callbacks.ModelCheckpoint(filepath='model/gru_multiinput.model', monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)]\n",
    "print('using time'+str(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# 绘制训练 & 验证的损失值\n",
    "def show_his(gru_his):\n",
    "    plt.plot(np.array(gru_his.history['loss']))\n",
    "    plt.plot(np.array(gru_his.history['val_loss']))\n",
    "    #plt.plot(np.array(gru_his.history['accuracy']))\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "def show_log(log):\n",
    "    plt.plot(log.losses)\n",
    "    plt.plot(log.val_losses)\n",
    "    plt.plot(log.aucs)\n",
    "    #plt.plot(np.array(gru_his.history['accuracy']))\n",
    "    plt.title('Model log')\n",
    "    plt.ylabel('Loss&auc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test','Val_auc'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_log(my_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-297f01cca949>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#打印模型图到本地\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgru_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sia_gru_with_exfeature_model.png'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshow_layer_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\gongjubao\\miniconda3_4.54\\envs\\huaweilab\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \"\"\"\n\u001b[1;32m--> 132\u001b[1;33m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\gongjubao\\miniconda3_4.54\\envs\\huaweilab\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rankdir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\gongjubao\\miniconda3_4.54\\envs\\huaweilab\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         raise ImportError(\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;34m'Failed to import `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;34m'Please install `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "#打印模型图到本地\n",
    "plot_model(gru_model, to_file='sia_gru_with_exfeature_model.png',show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#检测Qauc的程序，思路：把query_id相同的部分集群，然后计算其roc_auc_score，然后取所有的平均值\n",
    "#输入result（一位数组），真实表文件（包括两个id和正确的标签），将输出qauc值\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def metric_auc(predict,true):\n",
    "    start_time = time.time()\n",
    "\n",
    "    '''\n",
    "    true:      DataFrame , ['query_id','query_title_id','label']\n",
    "    predict:   np.array , [0.79,0.03,0.56,...]\n",
    "    '''\n",
    "    true = pd.concat([true.reset_index(drop=True),pd.DataFrame(predict)],axis=1)\n",
    "    true.columns = ['query_id','query_title_id','label','predict']\n",
    "    auc_score = []\n",
    "    count = 0\n",
    "    for i in tqdm(true.groupby('query_id'),mininterval=1.0):\n",
    "        a = i[1]\n",
    "        x = np.array(a['label'])\n",
    "        y = np.array(a['predict'])\n",
    "        try:\n",
    "            auc_score.append(roc_auc_score(x,y))\n",
    "        except:\n",
    "            auc_score.append(0.5)  \n",
    "            count+=1\n",
    "    print(count)\n",
    "    qauc = np.mean(auc_score)\n",
    "    if qauc<0.5:\n",
    "        qauc=1-qauc\n",
    "    return qauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "pred = gru_model.predict([train_query[-2000:],train_title[-2000:]])\n",
    "print('using time'+str(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1=[]\n",
    "lb1=[]\n",
    "print(type(label))\n",
    "for i in range(len(pred)):\n",
    "    pred1.append(pred[i][0])\n",
    "    #lb1.append(label[i][0])\n",
    "pred1=np.array(pred1)\n",
    "lb1=np.array(label)\n",
    "print(pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "true = pd.read_csv('valid.csv',header=None)\n",
    "print(true[:10])\n",
    "metric_auc(pred1,true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc_score(label[-2000:],pred1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
