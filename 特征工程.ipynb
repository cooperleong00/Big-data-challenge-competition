{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from gensim import corpora\n",
    "import scipy.stats as stats\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词向量特征\n",
    "#词向量训练的回调函数，用来打印loss、保存模型等等（注意，即使不训练词向量，加载时也要载入这个类）\n",
    "class train_log_word(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    def __init__(self, savename):\n",
    "        self.loss = []\n",
    "        self.time = 0.0\n",
    "        self.epoch = 0\n",
    "        self.savename = savename\n",
    "    #在每一轮开始的时候\n",
    "    def on_epoch_begin(self,model):\n",
    "        self.time = time.time()\n",
    "    #在每一轮结束的时候\n",
    "    def on_epoch_end(self,model):\n",
    "        self.epoch+=1\n",
    "        print('Epoch '+str(self.epoch)+' using time '+str(time.time()-self.time)+' seconds')\n",
    "        #这个loss似乎是总的loss叠加\n",
    "        print('loss: '+str(model.get_latest_training_loss()))\n",
    "        self.loss.append(model.get_latest_training_loss())\n",
    "        #5轮保存一次\n",
    "        if(self.epoch%5==0):\n",
    "            model.save(self.savename)\n",
    "            print(\"Model %s save done!\" % self.savename)\n",
    "        #50轮作图一次\n",
    "        if(self.epoch%50==0):\n",
    "            plt.plot(np.array(self.loss))\n",
    "            plt.title('Model loss')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            #plt.legend(['Train', 'Test'], loc='upper left')\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "#训练词向量,参数说明：存放corpus的文件，输出的文件名，轮数，词向量维数，中心词最远处理距离，线程数，过滤低于min_count的词，\n",
    "#sg=1为skip_gram训练方式、否则为cbow训练方式. alpha为初始学习率，min_alpha为最终学习率，学习率会逐渐降低至最终学习率.\n",
    "def make_w2v_model(corfile,output_model,\n",
    "                       epochs=100,vec_size=300,window=5,workers = 16,min_count=0,sg=1,alpha=0.025, min_alpha=0.0001):\n",
    "    start_time=time.time()\n",
    "    #读取语料文件为gensim需要的输入，行流式读取\n",
    "    cor=LineSentence(corfile)\n",
    "    #模型构建\n",
    "    model = gensim.models.Word2Vec(size=vec_size,window=window, workers=workers,sg=sg,min_count=min_count, \n",
    "                                   alpha = alpha,min_alpha = min_alpha)\n",
    "    #建立字典\n",
    "    model.build_vocab(cor)\n",
    "    #训练模型,注意这里使用了回调模型——train_log\n",
    "    model.train(cor,total_words=model.corpus_total_words,epochs=epochs,callbacks=[train_log_word(output_model)],compute_loss = True)\n",
    "    print(\"word2vec training complete,epochs:\"+str(epochs)+\" using time:\"+str(time.time()-start_time)+' seconds')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#段落向量特征\n",
    "#段落向量训练的回调函数，用来保存模型等\n",
    "class train_log_doc(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    def __init__(self, savename):\n",
    "        \n",
    "        self.time = 0.0\n",
    "        self.epoch = 0\n",
    "        self.savename = savename\n",
    "    #在每一轮开始的时候\n",
    "    def on_epoch_begin(self,model):\n",
    "        self.time = time.time()\n",
    "    #在每一轮结束的时候\n",
    "    def on_epoch_end(self,model):\n",
    "        self.epoch+=1\n",
    "        print('Epoch '+str(self.epoch)+' using time '+str(time.time()-self.time)+' seconds')\n",
    "        if(self.epoch%3==0):\n",
    "            model.save(self.savename)\n",
    "            print(\"Model %s save done!\" % self.savename)\n",
    "\n",
    "#训练段落向量（PV-DM或是PV-DBOW）\n",
    "#每个段落都映射到一个唯一的序列，由矩阵D中的一列表示，每个词也映射到一个唯一的向量，表示为W\n",
    "#对于某一段文本，当前段落向量和上下文所有词向量一起取平均值或者连接操作，生成的向量输入到softmax层，以预测上下文的下一个词\n",
    "#这个段落向量可以理解为一个词或是一个记忆单元，记住上下文缺失的内容或段落的主题。\n",
    "#段落向量不会跨段落（not across paragraphs），词向量对整个文档共享\n",
    "#PV-DBOW即句向量分布词袋模型。不把上下文中的词作为输入，而是强制模型从句子中随机抽取词汇来预测\n",
    "#PV-DBOW训练速度更稳定\n",
    "#参数说明（简单）：存放corpus的文件，输出的文件名，dm=1则为PV-DM模型，否则为PV-DBOW模型\n",
    "def make_d2v_model(corfile,output_model,epochs=1,vec_size=300,win_size=8,workers = 16,dm =1 ):\n",
    "    start_time=time.time()\n",
    "    #获取文档\n",
    "    documents = gensim.models.doc2vec.TaggedLineDocument(corfile)\n",
    "    #创建模型\n",
    "    model = gensim.models.Doc2Vec(size=vec_size,window=win_size, workers=workers,dm=dm)\n",
    "    #建立字典\n",
    "    model.build_vocab(documents)\n",
    "    #进行训练，callbacks回调函数\n",
    "    model.train(documents,total_words=model.corpus_total_words,epochs=epochs,callbacks=[train_log_doc(output_model)])\n",
    "\n",
    "    print(\"doc2vec training complete,epochs:\"+str(epochs)+\"using time:\"+str(time.time()-start_time))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 using time 0.7407498359680176 seconds\n",
      "loss: 450810.375\n",
      "Epoch 2 using time 0.7699403762817383 seconds\n",
      "loss: 799327.5\n",
      "Epoch 3 using time 0.9285156726837158 seconds\n",
      "loss: 1111550.125\n",
      "Epoch 4 using time 0.9065752029418945 seconds\n",
      "loss: 1392794.375\n",
      "Epoch 5 using time 0.8975985050201416 seconds\n",
      "loss: 1662556.625\n",
      "Model word2vec.model save done!\n",
      "Epoch 6 using time 0.8916149139404297 seconds\n",
      "loss: 1915821.125\n",
      "Epoch 7 using time 0.9315056800842285 seconds\n",
      "loss: 2147994.75\n",
      "Epoch 8 using time 0.9055790901184082 seconds\n",
      "loss: 2363680.25\n",
      "Epoch 9 using time 0.8816397190093994 seconds\n",
      "loss: 2566060.75\n",
      "Epoch 10 using time 0.9364910125732422 seconds\n",
      "loss: 2757453.0\n",
      "Model word2vec.model save done!\n",
      "Epoch 11 using time 0.8756585121154785 seconds\n",
      "loss: 2939869.0\n",
      "Epoch 12 using time 0.8816423416137695 seconds\n",
      "loss: 3119702.75\n",
      "Epoch 13 using time 0.8975989818572998 seconds\n",
      "loss: 3292234.0\n",
      "Epoch 14 using time 0.8776531219482422 seconds\n",
      "loss: 3458447.0\n",
      "Epoch 15 using time 0.8946073055267334 seconds\n",
      "loss: 3615676.25\n",
      "Model word2vec.model save done!\n",
      "Epoch 16 using time 0.8976011276245117 seconds\n",
      "loss: 3712319.25\n",
      "Epoch 17 using time 0.8766579627990723 seconds\n",
      "loss: 3856567.0\n",
      "Epoch 18 using time 0.874692440032959 seconds\n",
      "loss: 3998745.0\n",
      "Epoch 19 using time 0.8706691265106201 seconds\n",
      "loss: 4183907.75\n",
      "Epoch 20 using time 0.8606970310211182 seconds\n",
      "loss: 4307279.5\n",
      "Model word2vec.model save done!\n",
      "Epoch 21 using time 0.8566715717315674 seconds\n",
      "loss: 4387633.0\n",
      "Epoch 22 using time 0.8597021102905273 seconds\n",
      "loss: 4501950.5\n",
      "Epoch 23 using time 0.8696715831756592 seconds\n",
      "loss: 4655598.0\n",
      "Epoch 24 using time 0.8497278690338135 seconds\n",
      "loss: 4771589.5\n",
      "Epoch 25 using time 0.8497283458709717 seconds\n",
      "loss: 4875841.5\n",
      "Model word2vec.model save done!\n",
      "Epoch 26 using time 0.802851676940918 seconds\n",
      "loss: 4984794.5\n",
      "Epoch 27 using time 0.8497276306152344 seconds\n",
      "loss: 5089635.0\n",
      "Epoch 28 using time 0.8427467346191406 seconds\n",
      "loss: 5198048.5\n",
      "Epoch 29 using time 0.851722002029419 seconds\n",
      "loss: 5297735.0\n",
      "Epoch 30 using time 0.8387560844421387 seconds\n",
      "loss: 5403598.5\n",
      "Model word2vec.model save done!\n",
      "Epoch 31 using time 0.870671272277832 seconds\n",
      "loss: 5471909.0\n",
      "Epoch 32 using time 0.8457610607147217 seconds\n",
      "loss: 5572629.0\n",
      "Epoch 33 using time 0.9295139312744141 seconds\n",
      "loss: 5670269.0\n",
      "Epoch 34 using time 0.898629903793335 seconds\n",
      "loss: 5773046.0\n",
      "Epoch 35 using time 0.883636474609375 seconds\n",
      "loss: 5870580.5\n",
      "Model word2vec.model save done!\n",
      "Epoch 36 using time 0.8487308025360107 seconds\n",
      "loss: 5938006.0\n",
      "Epoch 37 using time 0.9135572910308838 seconds\n",
      "loss: 6037258.0\n",
      "Epoch 38 using time 0.8218023777008057 seconds\n",
      "loss: 6132043.5\n",
      "Epoch 39 using time 0.832772970199585 seconds\n",
      "loss: 6232272.5\n",
      "Epoch 40 using time 0.851722002029419 seconds\n",
      "loss: 6295101.0\n",
      "Model word2vec.model save done!\n",
      "Epoch 41 using time 0.8197765350341797 seconds\n",
      "loss: 6392560.0\n",
      "Epoch 42 using time 0.8267877101898193 seconds\n",
      "loss: 6489183.0\n",
      "Epoch 43 using time 0.8287849426269531 seconds\n",
      "loss: 6583730.5\n",
      "Epoch 44 using time 0.8517227172851562 seconds\n",
      "loss: 6676116.5\n",
      "Epoch 45 using time 0.8237974643707275 seconds\n",
      "loss: 6771069.5\n",
      "Model word2vec.model save done!\n",
      "Epoch 46 using time 0.8158230781555176 seconds\n",
      "loss: 6832648.5\n",
      "Epoch 47 using time 0.8098347187042236 seconds\n",
      "loss: 6927543.5\n",
      "Epoch 48 using time 0.8218040466308594 seconds\n",
      "loss: 7022266.5\n",
      "Epoch 49 using time 0.874661922454834 seconds\n",
      "loss: 7116665.0\n",
      "Epoch 50 using time 0.8277871608734131 seconds\n",
      "loss: 7208289.0\n",
      "Model word2vec.model save done!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec training complete,epochs:50 using time:45.892441511154175 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('110339', 0.653994083404541),\n",
       " ('71981', 0.5977658033370972),\n",
       " ('25485', 0.5974366068840027),\n",
       " ('4366', 0.5942772626876831),\n",
       " ('37166', 0.5938936471939087),\n",
       " ('34569', 0.5925248861312866),\n",
       " ('13168', 0.5915049314498901),\n",
       " ('24181', 0.5854678153991699),\n",
       " ('26626', 0.5841189622879028),\n",
       " ('86638', 0.5829948782920837)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#训练词向量\n",
    "w2v_model = make_w2v_model('query_title.txt','word2vec.model',epochs=50)\n",
    "w2v_model.wv.most_similar('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练段落向量\n",
    "d2v_model = make_d2v_model('query_title.txt','doc2vec.model',epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对词向量、段落向量进行读取\n",
    "w2v_model = Word2Vec.load(\"word2vec.model\")\n",
    "d2v_model = Doc2Vec.load('doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1480384.125\n",
      "Epoch 1 using time 0.9295544624328613 seconds\n",
      "loss: 112092.2890625\n",
      "Epoch 2 using time 0.9295587539672852 seconds\n",
      "loss: 220170.96875\n",
      "Epoch 3 using time 1.1598973274230957 seconds\n",
      "loss: 327874.28125\n",
      "Epoch 4 using time 1.30155348777771 seconds\n",
      "loss: 433103.375\n",
      "Epoch 5 using time 1.2796061038970947 seconds\n",
      "loss: 535727.25\n",
      "Model word2vec.model save done!\n",
      "Epoch 6 using time 1.3065037727355957 seconds\n",
      "loss: 635215.9375\n",
      "Epoch 7 using time 1.2975690364837646 seconds\n",
      "loss: 734141.1875\n",
      "Epoch 8 using time 1.3134841918945312 seconds\n",
      "loss: 830791.6875\n",
      "Epoch 9 using time 1.3154809474945068 seconds\n",
      "loss: 926942.4375\n",
      "Epoch 10 using time 1.2047746181488037 seconds\n",
      "loss: 1022488.5\n",
      "Model word2vec.model save done!\n",
      "1022488.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#词向量模型 继续训练（gensim继续训练的话，学习率会重新从默认值0.25开始，再慢慢掉到min_alpha，所以最好不要使用继续训练）\n",
    "my_call = train_log_word('word2vec.model')\n",
    "w2v_model.train(corpus_file='query_title.txt',\n",
    "total_words=w2v_model.corpus_total_words,\n",
    "epochs=10, \n",
    "compute_loss =True,\n",
    "callbacks=[my_call])\n",
    "\n",
    "# 观察最后一轮训练loss\n",
    "print(w2v_model.get_latest_training_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 41664.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete, using time 0.4870436191558838 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#特征1（1*2）：一句话中是否有重复的字词\n",
    "#注意：query和title分开使用\n",
    "def is_repeat(filename,outputfile,length):\n",
    "    with open(outputfile, 'w') as output:\n",
    "        with open(filename) as f:\n",
    "            #处理开始时间\n",
    "            start_time = time.time()\n",
    "            for i in tqdm(range(length),mininterval=1.0):\n",
    "                key = 0\n",
    "                line = np.array(f.readline().strip('\\n').split(' '))\n",
    "                #np.unique去重复值\n",
    "                line2 = np.unique(line)\n",
    "                if(len(line)>len(line2)):\n",
    "                    key = 1\n",
    "                output.write(\"{0}\\n\".format(key))\n",
    "            print(\"complete, using time \"+str(time.time()-start_time)+\" seconds\")\n",
    "    return True\n",
    "\n",
    "#调用该函数\n",
    "is_repeat('query.txt','query_repeat.txt',length = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:00, 211090.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make feature from train.csv using time 0.09774017333984375 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#特征2（3）：普通长度特征\n",
    "#长度特征生成方法，传入csv源文件,输出文件名，得到长度特征（包括query、title长度和它们的长度比）\n",
    "def len_feature(filename,outfile):\n",
    "    with open(outfile, 'w') as output:\n",
    "        with open(filename) as csv_file:\n",
    "            #处理开始时间\n",
    "            start_time = time.time()\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            for row in tqdm(csv_reader,mininterval=1.0):\n",
    "                #第一种方法，稍慢\n",
    "                #from tensorflow import keras\n",
    "                #len_query = len(keras.preprocessing.text.text_to_word_sequence(row[1],filters='',lower=True,split=' '))\n",
    "                #len_title = len(keras.preprocessing.text.text_to_word_sequence(row[3],filters='',lower=True,split=' '))\n",
    "                #第二种方法，第二种更快，但似乎占用内存更多，不过本来就是行流式读取，内存占用仍然很小\n",
    "                len_query = len(row[1].split(' '))\n",
    "                len_title = len(row[3].split(' '))\n",
    "                len_bi = len_query/len_title\n",
    "                output.write(\"{0},{1},{2}\\n\".format(len_query,len_title,len_bi))\n",
    "            print(\"make feature from \"+filename+\" using time \"+str(time.time()-start_time)+\" seconds\")\n",
    "    return True\n",
    "\n",
    "#调用该函数\n",
    "len_feature(\"train.csv\",'len_feature.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 20000/20000 [00:02<00:00, 6879.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete, using time 2.91686749458313 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#特征3（6*2）：一句话中的tfidf信息——平均值、最大值、最小值、数学期望、方差、中位数\n",
    "#得到corpus_tfidf，即词-tfidf的内容。输入train+query的文件，返回corpus_tfidf\n",
    "#注意：title使用，query_title使用\n",
    "def get_corpus_tfidf(filename):\n",
    "    sentence = LineSentence(filename)\n",
    "    dictionary = corpora.Dictionary(sentence)\n",
    "\n",
    "    #过滤掉频率最高的N个单词\n",
    "    #dictionary.filter_n_most_frequent(10) \n",
    "    #1.去掉出现次数低于no_below的 2.去掉出现次数高于no_above的。注意这个小数指的是百分数 3.在1和2的基础上，保留出现频率前keep_n的单词\n",
    "    dictionary.filter_extremes(no_below=2,keep_n=len(dictionary))\n",
    "    #生成corpus\n",
    "    corpus = [dictionary.doc2bow(text) for text in sentence]\n",
    "\n",
    "    # 初始化tfidf\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    # 转换整个词库\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    #corpus_tfidf.save('corpus_tfidf.txt')\n",
    "    return corpus_tfidf\n",
    "\n",
    "#得到tfidf的common feature，输入一个corpus_tfidf，输出到文件中\n",
    "def tfidf_common_feature(corpus_tfidf,outputfile):\n",
    "    start_time = time.time()\n",
    "    with open(outputfile, 'w') as output:\n",
    "        j = 0\n",
    "        for i in tqdm(corpus_tfidf,mininterval=1.0):\n",
    "        #if j ==5426:\n",
    "            #print(i)\n",
    "            need = np.array(i).T[1]\n",
    "            mean = need.mean()\n",
    "            nmax = need.max()\n",
    "            nmin = need.min()\n",
    "            #方差\n",
    "            var = np.var(need)\n",
    "            #数学期望\n",
    "            Ex = np.sum(need)\n",
    "            #中位数\n",
    "            middle = np.median(need)\n",
    "            output.write(\"{0},{1},{2},{3},{4},{5}\\n\".format(mean,nmax,nmin,var,Ex,middle))\n",
    "            j +=1\n",
    "            \n",
    "    print(\"complete, using time \"+str(time.time()-start_time)+\" seconds\")                \n",
    "    return True\n",
    "\n",
    "#调用函数\n",
    "corpus_tfidf = get_corpus_tfidf(r'query_title.txt')\n",
    "tfidf_common_feature(corpus_tfidf,'tfidf_common_feature.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:04, 4081.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated distance feature with 20000 lines, using time 4.905137300491333 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#特征4(9)：词向量计算出的段落向量与段落向量的距离\n",
    "#参数说明：词向量和段落向量的模型名，语料文件名和输出文件名\n",
    "def w2v_d2v_distance(w2vmodel,d2vmodel,corfile,outputfile):\n",
    "    #d2vmodel = Doc2Vec.load(d2v_model)\n",
    "    with open(outputfile, 'w') as output:\n",
    "        with open(corfile) as csv_file:\n",
    "            start_time = time.time()\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            line_count = 0\n",
    "            for row in tqdm(csv_reader,mininterval=1.0):\n",
    "                #返回具体行数的段落向量\n",
    "                d2v = d2vmodel.docvecs[line_count]\n",
    "                words = row[0].split(' ')\n",
    "                #创建300维的空np.array\n",
    "                w2v = np.zeros(300,dtype=float)\n",
    "                wl=len(words)\n",
    "                for word in words:\n",
    "                    w2v = np.add(w2v,w2vmodel.wv[word])\n",
    "                    \n",
    "                #取词向量的平均\n",
    "                w2v = w2v/wl\n",
    "                \n",
    "                #计算欧氏距离   （np.subtract:求差，np.square：平方）\n",
    "                eudi_dis = np.sqrt(np.sum(np.square(np.subtract(w2v,d2v))))\n",
    "                #计算cos距离     （np.vpot：点积）\n",
    "                cos_dis = np.vdot(w2v,d2v)/(np.sqrt(np.sum(np.square(w2v)))*np.sqrt(np.sum(np.square(d2v))))\n",
    "                #曼哈顿距离      (np.abs：绝对值)\n",
    "                mht_dis = 1.0/(1.0+np.sum(np.abs(np.subtract(w2v,d2v))))\n",
    "                #相异度距离\n",
    "                BC_dis = distance.braycurtis(w2v,d2v)\n",
    "                #坎贝拉距离\n",
    "                cb_dis = distance.canberra(w2v,d2v)\n",
    "                #皮尔森相关系数\n",
    "                corela = distance.correlation(w2v,d2v)\n",
    "                #海明距离(useless)\n",
    "                ham_dis= distance.hamming(w2v,d2v)\n",
    "                #杰卡德距离(useless)\n",
    "                jc_dis= distance.jaccard(w2v,d2v)\n",
    "                #方差加权距离\n",
    "                sqeudi_dis = distance.sqeuclidean(w2v,d2v)\n",
    "\n",
    "                \n",
    "                output.write(\"{0},{1},{2},{3},{4},{5},{6},{7},{8}\\n\".format(eudi_dis,cos_dis,mht_dis,BC_dis,cb_dis,corela,ham_dis,jc_dis,sqeudi_dis))\n",
    "                line_count +=1\n",
    "            print(\"calculated distance feature with \"+ str(line_count)+\" lines, using time \"+str(time.time()-start_time)+\" seconds\")\n",
    "    return True\n",
    "\n",
    "w2v_d2v_distance(w2v_model,d2v_model,\"query_title.txt\",\"w2v_d2v_distance.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [01:07, 298.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated w2v_sim_mat feature using time 67.02113509178162 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#特征5（3）：词向量的相似度矩阵（如果算力充足，甚至可以给title.txt也进行单独计算）\n",
    "#参数说明：传入词向量模型，语料文件名，输出文件名\n",
    "def w2v_sim_mat(w2vmodel,corfile,outputfile):\n",
    "    with open(outputfile, 'w') as output:\n",
    "        with open(corfile) as csv_file:\n",
    "            start_time = time.time()\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            #csv_reader可以用来读以空格作为分隔符的txt文件\n",
    "            for row in tqdm(csv_reader,mininterval=1):\n",
    "                #row此时是一个列表，只包含一个字符串\n",
    "                words = row[0].split(' ')\n",
    "                words_len = len(words)\n",
    "                #空矩阵\n",
    "                sim_mat = np.zeros((words_len,words_len))\n",
    "                #算法优化：遍历上三角矩阵，其他取对称\n",
    "                for i in range(words_len):\n",
    "                    for j in range(i,words_len):\n",
    "                        sim_mat[i][j] = w2vmodel.wv.similarity(words[i],words[j])#返回一个数（相似度）\n",
    "                        #sim_mat[i][j] = np.vdot(w2vmodel[words[i]],w2vmodel[words[i]])/(np.sqrt(np.sum(np.square(w2vmodel[words[i]])))*np.sqrt(np.sum(w2vmodel[words[j]])))\n",
    "                        sim_mat[j][i] = sim_mat[i][j]\n",
    "                #对每一行取平均值，返回一个一维数组\n",
    "                sim_mean = np.mean(sim_mat, axis=0)\n",
    "                sim_mean_mean = np.mean(sim_mean,axis=0)\n",
    "                #相似度矩阵平均值偏度\n",
    "                sim_mean_skew = stats.skew(sim_mean, axis=0)\n",
    "                #相似度矩阵平均值峰度\n",
    "                sim_mean_kurt = stats.kurtosis(sim_mean, axis=0)\n",
    "                \n",
    "                #下面是对整个矩阵进行操作，包括整个矩阵的偏度、偏度的平均、偏度的偏度、整个矩阵的峰度、峰度的平均等等（实际因为算力不足未使用）\n",
    "                #sim_skew = stats.skew(sim_mat, axis=0)\n",
    "                #sim_skew_mean = np.mean(sim_skew,axis=0)\n",
    "                #sim_skew_skew = stats.skew(sim_skew, axis=0)\n",
    "                #sim_kurt = stats.kurtosis(sim_mat, axis=0)\n",
    "                #sim_kurt_mean = np.mean(sim_kurt,axis=0)\n",
    "                output.write(\"{0},{1},{2}\\n\".format(sim_mean_mean,sim_mean_skew,sim_mean_kurt))\n",
    "            print(\"calculated w2v_sim_mat feature using time \"+str(time.time()-start_time)+\" seconds\")\n",
    "    return True\n",
    "\n",
    "#调用该函数\n",
    "w2v_sim_mat(w2v_model,\"query_title.txt\",\"w2v_sim_mat_mean.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:04, 4061.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated query&title distance feature using time 4.928260087966919 seconds\n"
     ]
    }
   ],
   "source": [
    "#特征6（9）：query的平均词向量和title的平均词向量间的距离\n",
    "#参数说明：传入词向量模型，源csv文件名，输出文件名\n",
    "def query_title_distance(w2vmodel,corfile,outputfile):\n",
    "    with open(outputfile, 'w') as output:\n",
    "        with open(corfile) as csv_file:\n",
    "            start_time = time.time()\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            line_count=0\n",
    "            for row in tqdm(csv_reader,mininterval=1.0):\n",
    "                query = row[1].split(' ')\n",
    "                title = row[3].split(' ')\n",
    "                query_len = len(query)\n",
    "                title_len = len(title)\n",
    "                query_w2v = np.zeros(300,dtype=float)\n",
    "                title_w2v = np.zeros(300,dtype=float)\n",
    "                for word in query:\n",
    "                    query_w2v = np.add(query_w2v,w2vmodel.wv[word])\n",
    "                for word in title:\n",
    "                    title_w2v = np.add(title_w2v,w2vmodel.wv[word])\n",
    "                query_w2v/=query_len\n",
    "                title_w2v/=title_len\n",
    "                \n",
    "                #计算欧氏距离   （np.subtract:求差，np.square：平方）\n",
    "                eudi_dis = np.sqrt(np.sum(np.square(np.subtract(query_w2v,title_w2v))))\n",
    "                #计算cos距离     （np.vpot：点积）\n",
    "                cos_dis = np.vdot(query_w2v,title_w2v)/(np.sqrt(np.sum(np.square(query_w2v)))*np.sqrt(np.sum(np.square(title_w2v))))\n",
    "                #曼哈顿距离      (np.abs：绝对值)\n",
    "                mht_dis = 1.0/(1.0+np.sum(np.abs(np.subtract(query_w2v,title_w2v))))\n",
    "                #相异度距离\n",
    "                BC_dis = distance.braycurtis(query_w2v,title_w2v)\n",
    "                #坎贝拉距离\n",
    "                cb_dis = distance.canberra(query_w2v,title_w2v)\n",
    "                #皮尔森相关系数\n",
    "                corela = distance.correlation(query_w2v,title_w2v)\n",
    "                #海明距离(useless)\n",
    "                ham_dis= distance.hamming(query_w2v,title_w2v)\n",
    "                #杰卡德距离(useless)\n",
    "                jc_dis= distance.jaccard(query_w2v,title_w2v)\n",
    "                #方差加权距离\n",
    "                sqeudi_dis = distance.sqeuclidean(query_w2v,title_w2v)\n",
    "\n",
    "                \n",
    "                output.write(\"{0},{1},{2},{3},{4},{5},{6},{7},{8}\\n\".format(eudi_dis,cos_dis,mht_dis,BC_dis,cb_dis,corela,ham_dis,jc_dis,sqeudi_dis))\n",
    "            print(\"calculated query&title distance feature using time \"+str(time.time()-start_time)+\" seconds\")\n",
    "    return True\n",
    "            \n",
    "#调用该函数\n",
    "query_title_distance(w2v_model,\"train.csv\",\"query_title_dis.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                        | 0/20000 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 30893.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using time :1.5665664672851562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                        | 0/20000 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 50027.90it/s]\n",
      "\n",
      "\n",
      "  0%|                                                                                        | 0/20000 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 1061164.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.69118564 0.30881436]\n",
      " [0.70821263 0.29178737]\n",
      " [0.70429092 0.29570908]\n",
      " ...\n",
      " [0.7534714  0.2465286 ]\n",
      " [0.75718997 0.24281003]\n",
      " [0.73717936 0.26282064]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                        | 0/20000 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 384025.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using time :0.5971519947052002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#特征7（1*2）：tfidf矩阵奇异值分解降维+预测（注：如果算力够，可以把title.txt单独拉出来也进行操作）\n",
    "#tfidf矩阵奇异值分解降维\n",
    "#参数说明：传入query和title的组合，输出文件名，需要分解至的维数\n",
    "def make_tf_idf_svd(corfile,outputfile,dim):\n",
    "    with open(outputfile, 'w') as output:\n",
    "        with open(corfile) as cor:\n",
    "            start = time.time()\n",
    "            #读取所有行组成大列表\n",
    "            train_cor = cor.readlines()\n",
    "            #生成字典\n",
    "            count_vect = CountVectorizer()\n",
    "            train_counts = count_vect.fit_transform(train_cor)\n",
    "            #生成tfidf\n",
    "            tfidf_transformer = TfidfTransformer()\n",
    "            train_tfidf = tfidf_transformer.fit_transform(train_counts)\n",
    "            #降维，需要30维\n",
    "            lsa = TruncatedSVD(dim) \n",
    "            tfidf_svd = lsa.fit_transform(train_tfidf)\n",
    "            \n",
    "            for i in tqdm(range(len(tfidf_svd)),mininterval=1.0):\n",
    "                #转换一下格式，使得成为逗号分隔符\n",
    "                need = str(list(tfidf_svd[i])).replace(' ','')[1:-1]\n",
    "                output.write(\"{0}\\n\".format(need))\n",
    "            print(\"using time :\"+str(time.time()-start))\n",
    "    return True\n",
    "\n",
    "#调用该函数\n",
    "make_tf_idf_svd(\"query_title.txt\",\"tfidf_svd.txt\",30) \n",
    "\n",
    "#用分解完了的特征向量，加上简单的分类器进行初步预测,预测值（lr和mh）作为特征\n",
    "#参数说明：上面分解完30维文件，标签，输出的文件名以及需要的（行数）\n",
    "def make_tfidf_predict(tfidffile,labelfile,outputfile,length):\n",
    "    with open(outputfile, 'w') as output:\n",
    "        with open(tfidffile) as train_data:\n",
    "            with open(labelfile) as label_data:\n",
    "                start = time.time()\n",
    "                train_x = []\n",
    "                #读进30维的训练数据\n",
    "                for i in tqdm(range(length),mininterval=1.0):\n",
    "                    train_x.append(list(map(float,train_data.readline().replace('\\n','').split(','))))\n",
    "                train_y = []\n",
    "                #读进标签数据\n",
    "                for i in tqdm(range(length),mininterval=1.0):\n",
    "                    train_y.append(float(label_data.readline()))\n",
    "                #简单分类器，使用它分别输出lr和mh，损失函数分别为'log'和'modified_huber'\n",
    "                clf = SGDClassifier(loss='modified_huber',early_stopping=True)\n",
    "                clf.fit(train_x,train_y)\n",
    "                #进行预测\n",
    "                pred = clf.predict_proba(train_x)\n",
    "                for result in tqdm(pred,mininterval=1.0):\n",
    "                    #result[1]取正向频率\n",
    "                    output.write(\"{0}\\n\".format(result[1]))\n",
    "            print(\"using time :\"+str(time.time()-start))\n",
    "            return True\n",
    "\n",
    "#调用该函数\n",
    "make_tfidf_predict('tfidf_svd.txt','lb.txt','tfidf_MH.txt',20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
